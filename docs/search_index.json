[["index.html", "Genomics for Precision Oncology 2025 Workshop Info Pre-work Class Photo Schedule", " Genomics for Precision Oncology 2025 Faculty: Daniel Gaston, Victor Martinez November 4-5, 2025 Workshop Info Welcome to the 2025 Genomics for Precision Oncology Canadian Bioinformatics Workshop webpage! Pre-work Before the workshop, please complete the following steps to ensure you are ready for the hands-on sessions: 1. Create a Franklin Account You will need a Franklin account for accessing genomics datasets and cloud-based tools used during the workshop. Go to https://franklin.genoox.com/join Register using your institutional email address (e.g., from your university, hospital, or research institute). Verify your email and log in to confirm your access. Once logged in, familiarize yourself with the interface — especially how to create a new workspace and upload a VCF file.  Tip: See Module 3, Part 1 for more detailed instructions. If you already have a Franklin account, make sure you can access it and remember your password before the workshop. 2. Download and Install IGV (Integrative Genomics Viewer) We will use IGV to visualize genomic variants and alignments during the lab sessions. Visit the official IGV download page: https://software.broadinstitute.org/software/igv/download Choose the installer appropriate for your operating system: Windows: Download the .exe file macOS: Download the .dmg file Linux: Download the .zip or use the command-line installer Follow the installation instructions on the site. Launch IGV to confirm it opens correctly. ⚠️ Note: IGV requires Java 11+. Most systems already have it, but if not, you can install OpenJDK from https://adoptium.net/.  Tip: See Module 4, Section 1 for more detailed instructions. Class Photo Coming soon! Schedule :root { --schedule-primary-color: #4A6BFF; --schedule-border-color: #EAECEF; --schedule-text-color: #555; } .schedule-card { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif; border: 1px solid var(--schedule-border-color); border-radius: 12px; margin: 2em 0; box-shadow: 0 4px 12px rgba(0,0,0,0.05); overflow: hidden; } .schedule-tabs { display: flex; justify-content: space-between; align-items: center; border-bottom: 1px solid var(--schedule-border-color); background-color: #fff; } .schedule-tabs-wrapper { position: relative; flex-grow: 1; overflow: hidden; } .schedule-tabs-wrapper::before, .schedule-tabs-wrapper::after { content: ''; position: absolute; top: 0; bottom: 0; width: 50px; pointer-events: none; transition: opacity 0.2s; opacity: 0; } .schedule-tabs-wrapper::before { left: 0; background: linear-gradient(to left, rgba(255,255,255,0), #fff 70%); } .schedule-tabs-wrapper::after { right: 0; background: linear-gradient(to right, rgba(255,255,255,0), #fff 70%); } .schedule-tabs-wrapper.scrolled-left::before { opacity: 1; } .schedule-tabs-wrapper.scrolled-right::after { opacity: 1; } .schedule-tab-buttons { display: flex; overflow-x: auto; white-space: nowrap; -ms-overflow-style: none; scrollbar-width: none; scroll-behavior: smooth; } .schedule-tab-buttons::-webkit-scrollbar { display: none; } .timezone-selector-container { display: flex; align-items: center; gap: 0.75em; padding: 0 20px; } .timezone-selector-container label { font-size: 0.9em; color: var(--schedule-text-color); font-weight: 500; } .schedule-tabs select { padding: 6px 10px; border-radius: 6px; border: 1px solid #ccc; background-color: #fff; } .schedule-tab { padding: 14px 20px; cursor: pointer; border-bottom: 3px solid transparent; margin-bottom: -1px; color: var(--schedule-text-color); font-weight: 500; } .schedule-tab.active { color: var(--schedule-primary-color); border-bottom-color: var(--schedule-primary-color); } .schedule-panel { display: none; } .schedule-panel.active { display: block; } .schedule-table { width: 100%; border-collapse: collapse; } .schedule-table th, .schedule-table td { text-align: left; padding: 14px 20px; border-bottom: 1px solid var(--schedule-border-color); } .schedule-table th { color: #888; font-size: 0.8em; font-weight: 600; text-transform: uppercase; background-color: #F8F9FA; } .schedule-table tr:last-child td { border-bottom: none; } /* --- RULE FOR BREAKS AND PAUSES --- */ .schedule-table tr.is-break td { font-style: italic; font-size: 0.95em; color: #777; } /* --- NEW: RULE FOR 'OTHER' TYPE (More Visible) --- */ .schedule-table tr.is-other td { background-color: #f0f4ff; /* A light blue background */ font-weight: 500; /* Slightly bolder text */ } .schedule-table tr.is-other td:first-child { border-left: 4px solid var(--schedule-primary-color); } "],["meet-your-faculty.html", "Meet Your Faculty", " Meet Your Faculty Instructors Dr. Dan Gaston (he/him) Clinical Bioinformatician/Assistant Professor NS Health and Dalhousie University Halifax, NS, Canada — daniel.gaston@dal.ca Dan Gaston is a Clinical Bioinformatician working at NS Health in the Molecular Diagnostics Lab and an Assistant Professor in the Department of Pathology at Dalhousie University. His work and research focuses on high-throughput genomic and transcriptomic profiling of solid tumours and hematological cancers of patients in Nova Scotia and the rest of Atlantic Canada. He also works on the design and application of bioinformatics tools in clinical care and translational research settings. Dr. Victor Martinez Clinical Genomics Specialist and Lead of Data Analytics/Assistant Professor IWK Health and Dalhousie University Halifax, NS, Canada Teaching Assistants Riley Arseneau (she/her) PhD Student Dalhousie University Halifax, Nova Scotia, Canada — Rarseneau@dal.ca Riley Arseneau (she/her) is a fourth-year PhD Student investigating the correlations between genetics, immune cells, and pancreatic cancer in the Boudreau lab at Dalhousie University. Sarah Russell (she/her) Genetic Analysis Specialist IWK Health Halifax, Nova Scotia, Canada — sarah.russell@iwk.nshealth.ca Sarah Russell is a Genetic Analysis Specialist at IWK Health, where she applies bioinformatics tools to support the clinical interpretation of genomic data. Her work focuses on developing and maintaining NGS workflows that ensure the accuracy and quality of genetic test results and clinical reporting. Her previous work involved the development and validation of bioinformatic solutions for somatic profiling of solid tumor panels used in clinical diagnostics. Facilitator Ben Fisher (he/him) Regional Coordinator, CBH Atlantic Canadian Bioinformatics Hub, Dalhousie University Halifax, NS, Canada — atlantic@bioinformatics.ca Ben has a Master of Science degree in Microbiology and Immunology, completing his bioinformatics training under Dr. Morgan Langille at Dalhousie University. Throughout his training, he has instructed others in genetics, molecular biology, and microbiome data science. Ben is passionate about continued education of trainees and professionals, and firmly believes that enhancing bioinformatics and computational biology competencies will support the success of Canada’s current and future scientists. "],["data-and-compute-setup.html", "Data and Compute Setup", " Data and Compute Setup Course data downloads Coming soon! Compute setup Coming soon! "],["module-1.html", "Module 1 Lecture", " Module 1 Lecture "],["module-2.html", "Module 2 Lecture Lab (Part 1) Lab (Part 2) - Implementing the GATK Best Practices Workflow Answers to Guiding Questions", " Module 2 Lecture Lab (Part 1) AWS and Unix Basics See Materials from pre-workshop session Docker Docker is what is known as a container service. Basically docker containers are like miniature versions of Virtual Machines, containing a base operating system and other code. Most often these exist to run specific programs in a controlled and self-contained environment, allowing you to run programs which may have complex software dependencies without needing to install them. In this lab many of the software tools you will use are ones where we will use the version that is distributed in a docker container. Docker itself could be the subject of an entire workshop, but getting started with Docker can be easy. For our purposes we will only need to be able to run a program via Docker and understand how Docker containers interact with the file system of the AWS virtual machine we are using. First, verify docker is installed: which docker Question: What is the path for the docker executable installed on your system? Docker containers come in the form of an image, these can be pulled (like code can be pulled with git) from remote repositories. A number of docker images have already been pulled for this AWS instance. You can see what images are already available on a system with the following command: docker image list There should be three images on the system already. These images are listed as repository/image name. Repositories can have a directory structure, and so everything to the left of the final / is the repository and directory path, while the string to the right of the terminal / is the image name. Images are also tagged with names that may provide version numbers or the names of what is known as a release. The most recent release of an image in a repository is also given the tage latest. Question: What are the names of the 3 docker images on the system? What repository did they come from? Can you use this informaton to find where on the internet the repository is for the two images that come from the same repository? Docker images/containers are run on a system using the docker run command. This takes on the following structure, many of these are optional arguments: docker run [docker options] [repository]/[image name]:[tag] [command] [command options] As an example, you can run the bwa command in its associated container with the following: docker run pegi3s/bwa bwa BWA is an example of a program that when run without any options it prints out a help message and exits, giving you information on how to run the program. These messages often provide other information as well, such as the version of the software being run. Question: What is the version of BWA being run with this docker container? One important aspect of Docker containers is that because they are self-contained pieces of code, they have their own filesystem “inside” the container. This exists in memory as the container is run, and is gone when the container stops running. Without some help a docker container doesn’t “see” your filesystem and can’t read or write files on that filesystem, it can only interact with files in its own system. In order for a docker container to interact with your filesystem, you need to mount parts of that filesystem inside the docker container as volumes. This is handled as an option to docker run with the -v command-line flag. This takes the following structure: docker run -v [host path]:[container path] [other docker options] [repository]/[image name]:[tag] [command] [command options] You can specify -v [host path]:[container path]`` multiple times to mount multiple different directories inside your running container. It is possible to run a container in interactive mode with the-itcommnad-line flag so you can navigate and work \"inside\" the container. This can be useful for understanding all of the software that may exist within the container as well as its internal filesystem structure. In this tutorial you will be given appropriate examples of volume mounting in order to mount/CourseDataand/workspace`. Investigating Some Genomics File Formats and Metadata FastQ Files For most Next-Generation sequencing instruments the primary output is in the form of FastQ files, which are most often encountered in their compressed form: .fastq.gz (remember that Linux doesn’t actually care about filename extensions and several alternatives such as .fq.gz are in common use). You can examine FastQ files, gzipped or not, from the command-line of a linux or MacOS system, with standard tools. One commonly used tool for viewing plain-text files from the command-line in chunks instead of loading the whole thing into memory (which would be a bad idea with msot FastQ files as they are very large) is the less command. When dealing with compressed files a version of the tool called zless is used instead. There are several small FastQ files we will use in this Lab in /home/ubuntu/CourseData/Module2/FastQs/. Note: these ‘z’ forms exist for many common command-line tools: grep/zgrep, less/zless, and cat/zcat among others. Note: If you want to hear a story of deeply nerdy humour, and why the tool less is called less, feel free to ask. Take a look at the files (one at a time) normal_test_1.fastq.gz and normal_test_1.fastq.gz in that directory using the zless tool. To page through the file hit the spacebar. zless normal_test_1.fastq.gz zless normal_test_2.fastq.gz Questions: How many base pairs are the sequencing reads? What is the instrument ID of the sequencer these sequencing reads were generated on? What sequencing lane were the reads from in the normal and the tumour? What were the flowcell IDs? Because of the structure of a FastQ File, you can use the number of lines in a file to determine the number of sequencing reads, zcat will output all of the lines to standard out, and we can pass that to wc -l to count the number of lines, dividing by 4 will give you the number of reads. This can be especially important to ensure that the Read 1 and Read 2 files are the same size. If they aren’t, something may be wrong with your data or it has come from a source where you have unpaired (singleton) reads. zcat normal_test_1.fastq.gz | wc -l Questions: How many reads are in the files? Are they all the same size? Because FastQ ID lines start with the @ symbol, you might think you could use the grep matching command to count the number of reads without needing to do any math. And on this particular data, it may actually work. Test it yourself: zgrep &quot;@&quot; normal_test_1.fastq.gz | wc -l Question:* Why would this be a bad idea in practice? BED Files BED (Browser Extensible data) files are used most often to represent genomic features. Many programs can use BED files to draw representations, like lines, on top of some sort of genomic data structure. For instance the UCSC Genome Browser, or IGV which you will use later on in this workshop. An example BED file can be found in /home/ubuntu/CourseData/Module2/accessory_files. A common use case for BED files is to give genomic ranges. The BED format didn’t have a formal specification until 2021! The original UCSC description of the file format is here. The formal 1.0 specification from the Global Alliance for Genomics and Health is here. You can print the full contents of this file to standard output: cat gene_target_regions.bed This is a minimal BED file with only the 3 required columns. Question: What does each column represent? Note: BED files use a 0-based coordinate system, such that the first base pair of a contig (typically a chromosome in our work) is a 0 instead of a 1. BED files are structured into column based data, where each column should be separated by a tab character. Tabs and other whitespace (like simple spaces) can’t be distinguished when we output a file like this, but mixing up tabs and spaces is a common source of files not behaving, or not behaving correctly, in genomics output. Unix lets us see these special characters, like tabs and new line characters. cat -A gene_target_regions.bed Question: What combination of characters are used to represent tabs and newlines? Make a copy of this file cp gene_target_regions.bed test.bed and then edit the copy you made using the command-line program nano: nano test.bed. This opens a simple text editor that you can navigate with your keyboard arrows but not with your mouse. Delete a tab and replace it with several spaces instead and repeat the cat -A command but on your edited test.bed file. Notice the difference between tabs and spaces when you view the file this way. BAM Files BAM files (Binary SAM), are another file format for containing information about sequencing reads. While we most often see these for storing alignment (short-read mapping) data, they can actually contain unaligned sequencing reads as well. In fact the newer versions of the GATK pipeline we are dealing with here often start by converting FastQ files to Unaligned BAMs. We aren’t going to do that here. BAM files are often (but not always) accompanied by an index file (*.bai). Because BAMs are compressed and generally sorted in some fashion (usuually by genomic coordinate but sometimesby Read Name), Index files allow other tools to efficiently move through and read that file. For instance by jumping directly to the part of a file where a genomic coordinate starts instead of reading it line by line. The SAM/BAM specification gives you the current format specifications of SAM and BAM files. Remember, a BAM is a compressed binary version of the SAM specification, which is encoded in plain text, although there are some differences, for instance SAM files use a 1-based coordinate system while BAMs are 0-based. Here we will use the tool samtools to explore BAM files. We have some example BAM files in /home/ubuntu/CourseData/Module2/BAMs All of the metadata associated with a file is found in its header. SAM/BAM files contain a number of different ‘tags’ like (RG?) and (SQ?) in the header. Information on what these tags are is found in section 1.3 of the linked specification PDF (page 3). View the header of your provided BAM file and reference the specification to interpret what each of these tagged sections is referencing. To view the header of the Normal sample in this directory: samtools head C-GIAB.normal.regions_of_interest.bam You’ll notice there is a lot of information here. You can page through all of the info by piping the output of this command to less, again using the spacebar to page through the file: samtools head C-GIAB.normal.regions_of_interest.bam | less You’ll see the sort order of a BAM file is specified with the SO tag. Question: What are the sort orders of the sample BAM files? After the (SQ?) tags, which give you the sequences the reads were aligned against, (PG?) tags give you different programs and command-lines that have been used on the data, appearing in the order they were used. In this case there have been some very lengthy command-lines with lots of options used on these files! Luckily the program is listed first with the ID field giving its name, and then all of the options Question: What programs were used on this file? We can use samtools to do lots of things. Most commonly we use it to sort files into different sort orders, to convert from SAM to BAM or vise versa, and can even use it to view alignments. To take a BAM file and “undo” the alignment process to create paired FastQ files, we will first need to change the sort order so that it is sorted by read name instead of coordinate sort order. samtools sort -n -@ 4 -O BAM -o [output_file_name.bam] [input.bam] Remember to specify paths if necessary for input and output files, and that the output filename doesn’t already exist. Specifying samtools sort -h will give you the list of options you can use with samtools sort. Question: What do the -n, -@ and -O command-line flags do? To output the read information back into FastQ format you use the samtools fastq command: samtools fastq -1 [output_file_name_1.fastq.gz] -2 [output_file_name_2.fastq.gz] -@ 2 [input_file.bam] Question: If you use zless to look at these FastQ files, what do you notice compared to the ones you looked at previously? VCF Files Variant Call Format (VCF) files are used to store information about genomic variants, a more generic term than mutation. The current specification for VCF format is here. Note: VCF files are 1-based coordinate systems. Keep this in mind when converting back and forth between BAMs, BEDs, and VCFs! Conversion tools will handle this for you, but this often trips people up when looking at mutations from VCF files in BAM files for instance, which you will be doing later! Like FastQs, VCFs may be encountered in either their compressed (*.gz) or uncompressed format. VCFs are typically compressed with bgzip, which is mutually compatible with the standard gzip command and uses the same file extension. BGzip is basically a special version of gzip that was created specifically for genomics file formats like the VCF. Like BAMs, compressed VCFs should be indexed (here the command/tool for indexing a compressed VCF is tabix). Note: If you just google the term VCF you may encounter an entirely different file format which was created for cell phones a long time ago. Sometimes you have to specify genomics or variant or similar in a google search term! Uncompressed VCFs can be examined using the less command, while gzip/bgzip compressed VCFs can be examined with zless. As with other genomic file formats the metadata of a VCF file is contained in the header. The header of a VCF file are all of the lines that start with one or more ‘#’ characters. The last of these is a line giving the column headers for the variant that follows. The variant data section of a VCF, like a BED file, is column-based, with each column separated by a tab character. The Header section of a VCF file contains information about various tags and fields in the VCF file. In addition, when a VCF file is annotated, all of the information about the annotations that are added to the INFO column of the data have their various tags explained here in the header. The various values that can be found in the FILTER column are also described here. An couple of example VCF files can be found in ~/CourseData/Module2/VCFs. One of these is an extremely minimal test VCF while the other is the output of MuTect2, a somatic variant caller you will use later in this workshop. The general structure of the VCF file is more apparent if you investigate the test_panel.GRCh38.minimal.vcf file first, before looking at the mutect2_tumour_normal.filtered.regions_of_interest.vcf.gz file. Question: What file format specification was used for each file? Is it the same or different? Question: In the Mutec2 generated VCF how many different values can the FILTER field take? Question: What is the ID in the FORMAT field for the Approximate read Depth at a site/variant? Question: What are the genomic coordinates of the first variant listed in the file? What are the Reference and Alt alleles? What entries are given in the filter field? Lab (Part 2) - Implementing the GATK Best Practices Workflow In this portion of the Lab we will be implementing a basic approximation of the Broad Institute’s [Best Practices Workflow] (https://gatk.broadinstitute.org/hc/en-us/sections/360007226651-Best-Practices-Workflows), which has been a standard pipeline for the analyis of human-derived next-generation sequencing data, particular whole genomes, for over a decade. There are in fact a number of best-practices workflows. Here we will focus on the Data pre-processing for variant discovery workflow. This is broadly applicable and takes data from FastQs or Unaligned BAMs to aligned BAMs that are ready for variant calling. This workflow is generally applicable and can feed into germline, somatic, or mitochondrial short variant calling, copy-number analysis, etc. Notably this is NOT appropriate for handling RNA-derived data such as a transcriptome. For the GATK portion of the lab we will be using the set of small FastQs found in the /home/ubuntu/CourseData/Module2/FastQs directory. These files only have 100,000 reads in them, so they will run through each step of the process very fast. That isn’t enough reads, when drawn from the whole genome, to do anything particularly useful. But some other example data is included in the ~/CourseData folder that can be used for visualization of BAMs with IGV or for running variant calling like MuTect2. FastQC Quality Control Prior to doing any actual analysis of data we receive from a sequencing experiment, we first have to perform so basic quality control to make sure the data is actually good. The FastQC program is a program that has been around “forever” in the NGS space for doing just that. This is one of the programs we will be using via a docker container. We are also going to want to make sure that our output goes into the ~/workspace directory of our AWS image as this directory is viewable in a web browser, which will let use view compatible output files as well as download data to our local machine. To run FastQC on the FastQ file with Read 1 data from the normal sample in the FastQs directory and output to a FastQC folder in the ~/workspace directory we will mount the home directory itself to the /data directory of the container. We need to also create the output directory first. The the ~/workspace directory create a folder caleld FastQC: mkdir FastQC. We can then run FastQC with docker. Note that this particular docker container automatically runs the fastqc command when the docker container is invoked, so it is not specified here in the command-line. docker run --rm -v /home/ubuntu/CourseData:/data -v /home/ubuntu/workspace:/workspace pegi3s/fastqc -o /workspace/FastQC/ /data/Module2/FastQs/normal_test_1.fastq.gz FastQC can even be run on more than one FastQ file at a time. Take a look at the command line options for FastQC by running docker run --rm pegi3s/fastqc -h and then modify the above command to run on all of the FastQ files in the FastQ directory. You can view the contents of your ~/workspace directory by going to: http://NUM.uhn-hpc.ca/ where NUM is the number for your particular AWS instance. You can click on any of the HTML files in the FastQC directory to open the FastQC reports directly. Question: How many reads are in the normal_test_1.fastq.gz file according to its FastQC output? How many total bases? Question: What is the GC % of th data? Question: What is the average quality per read? Alignment/Short-Read Mapping with BWA-MEM In this and following steps, the command-lines provided for you in code blocks: Like This May not be complete. They will server as guidance for building the complete and proper command-line yourself. The first step of the GATK workflow is to use a short-read mapping algorithm to align our FastQs to a Human Reference Genome (GRCh38). We will use the program BWA for this, again running BWA-MEM from a docker container. BWA-MEM as general input takes 1 or more FastQ files and a reference genome file, and then outputs the results to standard out in the uncompressed SAM format. We will re-route that output to a file. In this case, and for many of our intermediate processing steps, we will not output our results to the ~/workspace directory, but instead will output them to locations in the ~/CourseData directory. Since the outputs of one program will be the inputs of the next program in our chain this is generally more convenient. We will do all of our intermediate processing in the ~/CourseData/Module2/Processing directory. We also have a tumour and a normal sample in our FastQs, each of these steps, until we get to variant calling, we run on each of those individually. For convenience the command-lines provided below will assume you are running on the Normal tissue sample. Simply repeat, changing file names as appropriate for the tumour output. We can view all of the command-line options for BWA-mem by running the following command, which will also serve as the base for all of our BWA-MEM commands. docker run --rm -v &quot;/home/ubuntu/CourseData:/data&quot; pegi3s/bwa bwa mem We see the basic usage is: bwa mem [options] &lt;idxbase&gt; &lt;in1.fq&gt; [in2.fq]. In our case we have two FastQ files, which are gzipped (bwa-mem reads gzipped FastQs just fine). Our GRCh38 humans reference is located at: ~/CourseData/tools/reference/GATK-bundle-GRCh38/Homo_sapiens_assembly38.fasta. Question: What other files are located in the ~/CourseData/tools/reference/GATK-bundle-GRCh38/ directory? Hint: The various iterations on Homo_sapiens_assembly38.fasta with extra extensions are index files used by various programs to efficiently use the FASTA reference file. So the command-line structure for BWA-MEM (without the docker bits added on) looks like this to align two gzipped FastQ files to a reference FASTA file and output to a SAM file: bwa mem /path/to/reference.fasta /path/to/sample_1.fastq.gz /path/to/sample_2.fastq.gz -o /path/to/output.sam You have test FastQ files for the normal sample: /home/ubuntu/CourseData/Module2/FastQs/normal_test_1.fastq.gz /home/ubuntu/CourseData/Module2/FastQs/normal_test_2.fastq.gz and the Tumour sample: /home/ubuntu/CourseData/Module2/FastQs/tumour_test_1.fastq.gz /home/ubuntu/CourseData/Module2/FastQs/tumour_test_2.fastq.gz Create the command-line, using the docker elements and correct paths for docker to align the two normal FastQ files to the provided Human reference, and output to a SAM file in the ~/CourseData/Module2/Processing directory. Some hints, if you use the same volume mounting paths and names as I did above with this docker command docker run --rm -v \"/home/ubuntu/CourseData:/data\" pegi3s/bwa bwa mem then the normal FastQ files would have the following paths in the Docker container: /data/Module2/FastQs/normal_test_2.fastq.gz /data/Module2/FastQs/normal_test_1.fastq.gz and the processing directory path would be /data/Module2/Processing. Use Samtools to sort the results and convert a SAM to a BAM For further processing with the GATK we need to sort the SAM so that it is coordinate, and not read sorted, and convert it to a BAM file. We can do this from inside our ~/CourseData/Module2/Processing directory and we can do it in a single step: samtools sort -@ 4 -O bam -o normal.bam normal.sam Replace normal.bam and normal.sam with whatever you called your SAM file and whatever you want to call the BAM version of that file. Typically we would keep the file names the same and just change the extension. Repeat for the tumour SAM. Question: What command-line flag could you give to the samtools sort command above to have it automatically create an index file for the output BAM? Question: If you wanted to sort the SAM, but output to a SAM file instead how would you modify the file? What about if you wanted to create a BAM file but change its compression level to 0? Making a binary but uncompressed file? GATK Pre-Processing Aligned BAM files are not yet ready for variant calling, and some intermediate pre-processing needs to be done. Add Read Group Data First, we want to add Read Groups to our data which gives a sample name and information like Library and Lane IDs, as well as other metadata like the kind of sequencing that was used. Here we will use the GATK software package itself, which we are running from another docker container. Again the example below will just be for the normal sample, you’ll need to repeat it for the tumour. I will assume that your BAM file is just called normal.bam so you’ll need to change that and other possible parameters in order to match this to your own preferences. You can call files pretty much anything you want after all, but I will use some naming conventions here that I find helpful for tracking where in the workflow a file was produced. You can read more about Read Groups here. Also in the SAM specification that you were provided earlier. The Basic Docker part of your command-line for the GATK should be: docker run -v /home/ubuntu/CourseData:/gatk/CourseData --rm public.ecr.aws/aws-genomics/broadinstitute/gatk:4.2.6.1-corretto-11 gatk I will assume this portion in the command-lines below and replace that with \\[docker gatk\\]. Remember that the [] here is not meant to by typed literally, this is indicating a variable where you will replace it with the appropriate value, like the docker associated command-line above. [docker gatk] AddOrReplaceReadGroups -I /gatk/CourseData/Module2/Processing/normal.bam -O /gatk/CourseData/Module2/Processing/normal.rg.bam --RGID normal_lane1 --RGLB normal_lane1 --RGSM normal --RGPL ILLUMINA --RGPU Illumina You can put whatever you want in the various RG tabs you are adding. Here I am giving Read Group IDs (RGID) and lanes the same value, and specifying the data came from Lane 1 of the Flowcell. You can alter this to whatever the Lane was that the normal and tumour came from in their respective files for accuracy. I have also given it the sample name of ‘normal’. In the tumour you would want to use –RGSM tumour. This will be important for variant calling later on! Modify this command-line however you need to, and also modify and run again for the tumour sample. Set Nm, Md, and Uq tags The next step runs the SetNmMdAndUqTags tool. This isn’t always required, but some upstream tools in similar workflows or ways of handling and merging BAM files may mean these particular tags aren’t set. I have found it to be good practice to run it here. These tags report the number of mismatches, mismatching positions, and the unique quality of our alignments compared to the reference sequence. [docker gatk] SetNmMdAndUqTags -I /gatk/CourseData/Module2/Processing/normal.rg.bam -O /gatk/CourseData/Module2/Processing/normal.fixed.bam -R /gatk/CourseData/tools/reference/GATK-bundle-GRCh38/Homo_sapiens_assembly38.fasta MarkDuplicates PCR duplication rates vary, but even at their lowest are around 1-2%, so even in our 100,000 test reads we probably have a few. Either way, for whole genome sequencing workflows this is a crucial data quality control step. Note: While we refer to this step colloquially as “Duplicate Removal” we are actually only marking them in this step. The duplicate reads are still in the BAM file, but a flag is set that marks them as being a PCR duplicate. Therefore this is what we call a lossless step. The preferred method of doing this is with the MarkDuplicatesSpark tool; however, because of the java version in use in our container we need to use the non-spark version with this test setup. The command to invoke them is nearly identical. MarkDuplicatesSpark scales better on real data as it can use multiple CPU threads. But for our purposes this works just as well. [docker gatk] MarkDuplicates -I /gatk/CourseData/Module2/Processing/normal.fixed.bam -O /gatk/CourseData/Module2/Processing/normal.dup_marked.bam -M /gatk/CourseData/Module2/Processing/normal.duplicate_metrics.txt Question:: What information is found in the duplicate_metrics.txt file? How many Duplicates were marked by this process? Recalibrated Base Qualities The Base Quality Scores that come from Illumina Sequencing data are good, but the GATK uses several models based on the data to recalibrate these based on their own models as well as the data in your alignment. This step works best with Whole Genome Sequencing. This is a step that runs in two parts, first we do a calculation that creates a recalibration table, and then we apply it. The recalibration here uses some additional reference files, VCF files that contain variant information from dbSNP and GnomAD. Two databases of population allele references. This gives us locations of known polymorphisms in the human genome which is an input to the model. [docker gatk] BaseRecalibrator -I /gatk/CourseData/Module2/Processing/normal.dup_marked.bam -R /gatk/CourseData/tools/reference/GATK-bundle-GRCh38/Homo_sapiens_assembly38.fasta --known-sites /gatk/CourseData/tools/reference/GATK-bundle-GRCh38/Homo_sapiens_assembly38.dbsnp138.vcf --known-sites /gatk/CourseData/tools/reference/GATK-bundle-GRCh38/af-only-gnomad.hg38.vcf.gz -O /gatk/CourseData/Module2/Processing/normal.bqsr_recal.table Now apply it: [docker gatk] ApplyBQSR -R /gatk/CourseData/tools/reference/GATK-bundle-GRCh38/Homo_sapiens_assembly38.fasta -I /gatk/CourseData/Module2/Processing/normal.fixed.bam --bqsr-recal-file /gatk/CourseData/Module2/Processing/normal.bqsr_recal.table -O /gatk/CourseData/Module2/BAMs/normal.recalibrated.bam At this point we have a complete and pre-processed BAM file ready for variant calling! Question: We changed the output here for the final recalibrated BAM file as this is a ‘final’ output. Why is this good practice? Alternatives to Try Some tools can be run in different modes or with differe t options, and are worth exploring. For instance, it is possible to add Read Group information while running bwa mem with the -R flag flollowed by a string for adding the information: -R &#39;@RG\\tID:foo\\tSM:bar&#39; Try taking the RG tags you added with the GATK tool AddOrReplaceReadGroups and use this option with BWA MEM to add them in one go. You can them use samtools head to view the headers of your files and compare the (RG?) sections. Try taking this sam, convering it into a BAM, and then proceeding with downstream GATK tools. You can skip the AddOrReplaceReadGroups step, adjust your BAM file names accordingly. You can also try skipping the SetNmMdAndUqTags step. Question: What does your command line look like? Putting Everything Together Running commands one-by-one on the command-line is the best way to learn the ins and outs of a process and how to use specific tools. However, if we are going to do this more than once we want our work to be reproducible, which means putting things together as a script that we can simply configure and run whenever we need it. When you hear a bioinformatician refer to a pipeline that is all they really mean. A bit of code infrastructure that automates a process, taking inputs through a series of steps to get a final output. In the pre-work for this course you learned about simple shell scripting. While there are more complex ways of implementing a pipeline, these are usually only needed when you are going to run lots of data and want robust error handling and other features. If you are only going to occasionally run a task, are working primarily on a server like this one, and don’t need to handle a lot of complex data then a simple shell script will suffice. Take the commands you have execute so far and put them into a basic shell script. To make it more useful so that you don’t have edit all your commands to change file names everytime you want to run it you should look for all of the places that you can substitute out values for variable names. That way you can simply change variable names as appropriate. The Module2 directory structure is a good one for this kind of work. It organizes inputs (FastQs and accessory files), gives locations for final outputs (BAMs and VCFs), and has a central Processing directory whose contents can be safely deleted once the final outputs are created. The best targets for variables are: - Paths to specific directories, especially a ‘base’ working directory of some kind. For instance /home/ubuntu/CourseData/Module2 here. - Sample names or IDs that can be substituted into file names - Paths to specific files that get used a lot, like for instance the GRCh38 reference sequence - Long strings of supporting code that gets used again and again, like some of the docker wrapping code used here for many programs. Some other pointers for good script writing: - Scripts execute the commands inside them ‘silently’, so it is good practice to use echo in your scripts to output a command you are executing to standard output before executing. - Outputting some basic information at the top of a script, like printing out what sample name or IDs are being used, references, binaries, etc can also be useful - Repeat exact code as little as possible to minimize the number of places you need to make changes for fixing bugs or altering code - Define as many variables as possible at the top of the code - Try and keep all the places you need to make edits for having the script run on a different sample be at the top Once you have a shell script you should be able to execute it: sh [scriptname] Viewing BAMs in IGV Some additional BAMs, made from the same data source as what we have been using so far, have been provided in the ~/CourseData/Module2/BAMs directory. Here a complete alignment of all of the data was made, and then the BAM was “subset” so that it only contained data from a few regions of the genome. The regions of interest are the ones specified in the BED file we investigated earlier. We will investigate these BAMs in IGV to see what genes were included in our BAM. The Integrative Genomics Viewer (IGV) is a tool primarily used for viewing BAM files so you can visually see how the reads are aligned, the context of mutations, etc. You can download the program here. While there is an alternative version that runs in a browser window without installing, in my opinion that should only be used in the last resort. At the Download link there are a number of versions available depending on your operating system and CPU chipset in the case of MacOS versions. Go ahead and download and install IGV on your computer. In order to make downloading the BAM files in the ~/CourseData/Module2/BAMs directory and their associated index files easy, we will create what is known as a soft-link to them in the ~/workspace directory so you can download them from the browser. Here is the general form of the command: ln -s /path/to/original_file_or_directory /path/to/symlink Run for each of the BAMs and BAI files in the Module2 BAMs directory and also do it for the BED file in the ~/CourseData/Module2/accessory_files directory. You can then download them via the browser like how you viewed the FastQC outputs previously. First make sure the correct reference sequence is selected in the drop down menu in the upper left. The default for a new installation should be the human GRCh38 reference, which is what we want to use. IGV can be used with any reference, not just human sequences so if you ever find yourself doing microbial or viral genomics, IGV is still a useful tool! Learning all of the ins and outs of IGV is beyond the scope of this lab; however, using it to view the basics of a BAM file only requires default options. When you have IGV installed and running, you can open files in it with File -&gt; Load From File. You can use this to load BAM files, and you can open multiple files at once. Navigate to where you downloaded the BAMs and thier BAI index files, select both BAM files (no need to select the BAIs) and then open them. Files load as Tracks in IGV, on the very bottom we have a representative of the reference sequence. Each BAM loads two tracks, one for the alignments themselves (only viewable when zoomed in to an appropriate resolution level) and one for plotting the coverage. We can also load VCF files, which will then indicate mutations from that file as their own Track, and BED files which will plot the ranges from that file in their own Track. VCFs will tend to load as tracks at the top while BEDs will load at the bottom. Use File -&gt; Load From File to open the BED file you downloaded as well, which plots the targeted regions of the genome that are of interest. There are two easy ways to navigate to particular regions of the genome in IGV. You can type in a gene name, like KRAS, or specify genomic coordinates (like that are in your BED file) with the format: chr9:1000-1500. You can then change zoom levels in and out to view your alignment. Try navigating to the KRAS gene and also to each of the genomic coordinates in the BED file to inspect your BAM. By default IGV will show base pairs where the base in the sequencing read and that in the reference genome match as just a grey block. Mismatches will have the base shown and coloured, insertions in the read show up as an I character, and deletions as a - character. Question:: What genes of interest were included in the regions selected for your BAM files? Which of these are tumour suppressors and which are oncogenes? You will use IGV more heavily tomorrow, epsecially when reviewing data for the Cases! Somatic Variant Calling with MuTect2 If you have time, you can continue on with Variant Calling and Variant Annotation, which will get you started for Day 2! Once both your tumour and normal Recalibrated BAM files are produced, you are ready for Variant Calling. However, the simple test data we have used so far won’t produce any mutation calls. After all we only aligned 100,000 sequencing reads across the entire human genome. Instead we will use the more complete BAMs you investigated with IGV above for variant calling. MuTect2 is included in the GATK package and is run the same way, with the same docker-associated code on your command-line. Variant calling is more resource intensive on the RAM usage side, and so the java machine in the docker container needs to be given some more specific options to make use of the RAM. In this case we are going to give it 12 gigs of RAM (our virtual machine has 16) with the --java-options \"-Xmx12G\". MuTect2 needs a Reference FASTA file like we have used previously, and of course when running any sort of genomics analysis you will always use the exact same reference throughout. MuTect2 can be run in a few different modes, and is a somatic variant caller optimized for tumour data. It can run with a tumour-only or with matched tumour-normal data. This second option is what we will be using. To run in this mode you specify the -I input file flag twice, giving it both the tumour BAM and the normal BAM, and then use the -normal flag to give it the name of the normal sample. This needs to match the Sample (SM) Read Group tag you have during the GATK pre-processing steps early, in this case ‘normal’ was the string we used. MuTect2 can be run with lots of different options (see here). In this case we will also be giving it a germline resource VCF with the --germline-resource command-line parameter and are using the GnomAD set here, which contains allele frequency information for polymorphisms and mutations seen in tens of thousands of genomes. [docker gatk] --java-options &quot;-Xmx12G&quot; Mutect2 -R /gatk/CourseData/tools/reference/GATK-bundle-GRCh38/Homo_sapiens_assembly38.fasta -I /gatk/CourseData/Module2/BAMs/C-GIAB.tumour.regions_of_interest.bam -I /gatk/CourseData/Module2/BAMs/C-GIAB.normal.regions_of_interest.bam -normal normal --germline-resource /gatk/CourseData/tools/reference/GATK-bundle-GRCh38/af-only-gnomad.hg38.vcf.gz -O /gatk/CourseData/Module2/VCFs/mutect2_tumour_normal.regions_of_interest.vcf.gz After running baseline somatic variant calling with MuTect2 we can also apply the FilterMutectCalls program. This can incorporate various additional pieces of information for Read Bias artifacts when dealing with FFPE data or information about potential contamination. We have neither of those here but will run FilterMuTectCalls anyway. Note: The output file name provided here will overwrite the existing VCF in the VCFs directory. You should change its name to be unique, or output it to the workspace directory! [docker gatk] --java-options &quot;-Xmx12G&quot; FilterMutectCalls -R /gatk/CourseData/tools/reference/GATK-bundle-GRCh38/Homo_sapiens_assembly38.fasta -V /gatk/CourseData/Module2/VCFs/mutect2_tumour_normal.regions_of_interest.vcf.gz -O /gatk/CourseData/Module2/VCFs/mutect2_tumour_normal.filtered.regions_of_interest.vcf.gz The output here is now a ‘complete’ VCF. Variant Annotation with VEP Called variants on their own are not generally the most useful. If we want to gather information like what Gene a mutation is in, what the impact of a mutation is at the transcript and protein levels, whether a mutation is a known polymrphism in the GnomAD database or a known pathogenic mutation in ClinVar, we have to add annotations. The annotator we will use here is VEP (Variant Effect Predictor) from Ensembl (Documentation). We won’t be using a docker container here, but something similar called apptainer. You can see it works much the same way, although the -B flags here are used instead of -v. Here we are going to annotate using a GRCh38 reference set, and are telling VEP to give us ‘everything’. VEP can be modified with all sort sof plug-ins and modules that go beyond what is added here, but this will be plenty to get you started looking at annotations! apptainer exec -B /home/ubuntu/CourseData/Module2/VCFs:/data -B /media/cbwdata/CourseData/tools/vep_data:/cache ~/CourseData/tools/ensembl-vep.sif vep -i /data/test_panel.GRCh38.minimal.vcf -o /data/test_panel_GRCh38.minimal.vep.txt --cache --dir_cache /cache --species homo_sapiens --assembly GRCh38 --everything Answers to Guiding Questions "],["module-3-lab-clinical-variant-interpretation-with-franklin.html", "Module 3 Lab: Clinical Variant Interpretation with Franklin Learning Objectives Lab Overview Part 1: Account Creation and Platform Navigation Part 2: Exploring Variant Classification Part 3: Uploading and Classifying Your Case VCF (45 minutes) Part 4: Variant interpretation Part 5: Extension Exercise (Optional) Summary", " Module 3 Lab: Clinical Variant Interpretation with Franklin Summary: This lab focuses on clinical variant interpretation using the Franklin platform, emphasizing ACMG classification criteria. Participants will create accounts, navigate the interface, and classify variants from a hereditary cancer case. Key learning objectives include distinguishing automated predictions from manual curation, interpreting population frequency data, and understanding the classification process. The lab highlights the importance of human input in variant interpretation despite automation, the role of phenotype matching, and the value of literature curation in clinical settings. Text: Clinical Variant Interpretation with Franklin Learning Objectives By the end of this lab, you will: Navigate a clinical variant interpretation platform used in diagnostic laboratories Apply ACMG classification criteria to real variants Distinguish between automated predictions and manual curation requirements Interpret population frequency data, in silico predictions, and clinical evidence Lab Overview This lab shifts focus from computational variant calling to clinical interpretation. You’ve seen how variants are detected algorithmically, and now you’ll learn how clinical labs determine which variants matter. We’re using a hereditary cancer case because these variants illustrate key principles that apply to both germline and somatic contexts. The variant classification process you’ll practice here is what molecular geneticists perform daily when reporting diagnostic results. Why Franklin? Franklin (by Genoox, now Qiagen) offers free academic accounts with full VCF upload capability. More importantly, it provides surprisingly accurate automated ACMG code assignment. The platform applies the same evidence rules used in certified diagnostic labs, which makes it a realistic training environment. You’ll review each assigned ACMG criterion, adjust evidence strength when appropriate, and understand why certain variants remain difficult to classify. Part 1: Account Creation and Platform Navigation Creating Your Franklin Account Step 1: Access the registration page Navigate to: https://franklin.genoox.com Click “Sign Up” in the upper right corner. Step 2: Register with your institutional email Fill in the registration form: Email: Use your university/institutional email (required for academic access) Password: Standard requirements (8+ characters, mixed case, number) Click “Continue” First and Last Name: Your actual name (this appears in case reports) Organization Name: Your affiliated organization (You can create any) Professional Role: Select what fits better for you Important: Check your email for a verification link. Some email systems flag this as spam, so check your junk folder if you don’t see it within 5 minutes. You now have access to the full platform. Franklin operates on a “freemium” model (the free tier includes unlimited cases and full ACMG classification tools, which is everything we need for this workshop). Platform Navigation: Understanding the Main Interface When you first log in, Franklin displays the Search page as your landing screen. The interface is organized into three main sections accessible via the top navigation banner. Top Navigation Tabs: 1. Search (default landing page) Central search functionality for querying variants across all accessible cases Allows you to search by variant nomenclature, gene name, or genomic coordinates Returns results from your cases and (if enabled) the community variant database 2. Knowledge Base Contains pre-defined gene panels for common clinical indications Allows creation of custom gene panels for research or clinical use Links to curated gene-disease relationships and dosage sensitivity information Useful for batch analysis of variants within specific gene sets 3. My Cases Lists all cases you’ve created or been granted access to Shows case status (In Progress, Classified, Reviewed) Searchable by patient identifier, gene, or variant Supports team-based workflows (see below) Collaborative Features: Franklin supports team-based variant interpretation. You can: Invite colleagues to review specific cases Assign roles (curator, reviewer, approver) Track who made classification decisions and when Add internal comments and discussion threads This is particularly valuable in clinical settings where variant interpretation requires consensus among multiple specialists. Creating and Searching for Variants The Central Search Bar The search interface occupies the middle of the screen. Here you can manually enter variants or search existing classifications. Understanding Variant Nomenclature: Click on “Examples” near the search bar to view supported nomenclature formats: SNP (Single Nucleotide Variants): Uses HGVS notation Example: MSH6:c.4082delA Example: chr2:47641560-C-T CNV (Copy Number Variants): Genomic coordinates format Example: DEL:chr1-216138600-216270555 Example: arr\\[GRCh37\\] 7q31.2(116714478_116715973)x1 ROH (Runs of Homozygosity): Region-based notation Used for consanguinity assessment or uniparental disomy detection Reference Genome Build Selection: Critical setting located next to the search bar: Dropdown menu displays current reference build (default may be hg19 or hg38) For this workshop: Ensure “hg19” (GRCh37) is selected Mismatched genome builds will cause variant coordinates to be incorrect Case Type Selection: Next to the reference build selector, choose the variant analysis framework: Germline: Applies ACMG/AMP germline variant classification criteria (what we’ll use today) Somatic: Applies AMP/ASCO/CAP somatic variant interpretation guidelines (for cancer tumour variants) The case type determines which evidence codes and classification rules Franklin applies. For hereditary cancer variants, select “Germline” even though the gene is cancer-related. Creating New Cases Below the search bar, three options allow case creation from different data types: 1. CMA (Chromosomal Microarray Analysis) Upload microarray data files (.CEL, .CHP, or processed CNV calls) Generates CNV calls and quality metrics Primarily used for constitutional disorders and developmental delay cases 2. Report Manual entry of a single variant or small variant list Useful when you have variant calls from external pipelines Requires HGVS notation or genomic coordinates 3. VCF Upload variant call format files from sequencing pipelines Supports both single-sample and multi-sample VCFs Franklin automatically filters and prioritises variants This is the method we’ll use at the end of this section. If you prefer to start uploading your VCF now, you can skip ahead to Part 3, Step 2 for detailed instructions on case creation and file upload. Understanding Franklin’s Automated Classification Logic Franklin scans multiple databases and assigns ACMG evidence codes based on specific thresholds defined in the 2015 ACMG/AMP guidelines: Population frequency (PM2, BA1, BS1): Queries gnomAD, ExAC, 1000 Genomes. If a variant is absent in these databases, it automatically assigns PM2 (moderate evidence of pathogenicity). If frequency exceeds 5%, it assigns BA1 (benign standalone). Computational predictions (PP3, BP4): Aggregates SIFT, PolyPhen-2, CADD, REVEL, and others. If multiple algorithms agree on “deleterious,” it suggests PP3. If they agree on “benign,” it suggests BP4. Functional consequence (PVS1, PM4): Recognises null variants (nonsense, frameshift, canonical splice sites) in loss-of-function intolerant genes and assigns PVS1 (very strong pathogenicity). In-frame indels in critical regions get PM4. Known pathogenic variants (PS1, PM5): Cross-references ClinVar. If your variant matches a known pathogenic entry at the same codon or amino acid, Franklin flags PS1 or PM5. What requires manual curation: Segregation data (PP1, BS4): You need to enter family history and co-segregation evidence manually. Functional studies (PS3, BS3): If you have lab data showing altered protein function, you add this evidence yourself. Allelic data (PM3, BP2): Whether a variant is in trans with a known pathogenic variant (for recessive conditions). De novo status (PS2, PM6): Requires parental testing confirmation, which you document manually. Clinical correlation (PP4): Patient phenotype matching gene-disease associations. Franklin’s automation handles about 60-70% of the evidence gathering. The remaining 30-40% depends on clinical context that only you can provide. This is why variant interpretation remains a skilled professional task despite computational advances. Part 2: Exploring Variant Classification Before uploading, you start working on your case-specific VCF file, we’ll quickly explore Franklin’s classification workspace using a demonstration variant. This walkthrough uses an MSH2 variant to show you the complete evidence evaluation process. Step 1: Searching for a Variant To demonstrate Franklin’s classification interface, we’ll search for an existing variant in the MSH2 gene (a mismatch repair gene associated with Lynch syndrome). Procedure: Ensure you’re on the Search page (default landing screen) In the central search bar, type: MSH6:c.4082del Verify your settings before searching: Reference build: hg19 (GRCh37) Case type: Germline Press Enter or click the search button. Franklin will query its database and display the variant classification page if this variant exists in the system. If you’re creating a new variant assessment, you would instead click “Create New Case” and the platform would generate a fresh classification workspace.  Note: The variant notation here uses HGVS coding sequence nomenclature (c. prefix indicates coding DNA). Franklin accepts multiple notation formats, but HGVS is the clinical standard. Step 2: Understanding the Classification Workspace When Franklin loads a variant, the page organises into several key sections. Understanding this layout helps you efficiently navigate the evidence. Page Header (Top Section): Variant identifier: Displays the genomic coordinates, reference/alternate alleles, and HGVS notation Gene and transcript: Shows which gene is affected and which transcript Franklin is using (typically the MANE Select transcript) Suggested classification: Franklin’s automated prediction based on accumulated evidence (VUS/Likely Pathogenic/Pathogenic/Likely Benign/Benign) Classification scale: A visual “thermometer” showing where the variant falls on the benign-to-pathogenic spectrum Main Content Tabs: The workspace has multiple tabs, but you’ll primarily use: Franklin ACMG Classification: The main evidence evaluation workspace (this is where you’ll spend most of your time) Variant Assessment: Additional computational predictions and population frequency details Publications: Linked literature from PubMed Gene Assessment: Gene-level information (constraint scores, disease associations, dosage sensitivity) Step 3: Reviewing Automatically Assigned ACMG Codes The Franklin ACMG Classification tab displays all 28 ACMG evidence criteria organised into categories. Each criterion shows: Evidence code: (e.g., PM2, PVS1, PP3, BA1) Description: Brief explanation of what the code represents Status: Whether Franklin automatically assigned this code Strength: The evidence level (Very Strong, Strong, Moderate, Supporting) Understanding the Evidence Display: Franklin uses colour coding to indicate code status: Red badge (Pathogenic codes like PM2, PVS1): Assigned and contributing points toward pathogenic classification Green badge (Benign codes like BA1, BP4): Assigned and contributing points toward benign classification Grey/collapsed codes: Not automatically assigned; may require manual evaluation The “Unmet” codes section at the bottom lists criteria that Franklin could not evaluate automatically. These typically require clinical information that you must provide manually. Step 4: Deep Dive into Population Data (PM2 Example) Let’s examine how Franklin evaluates population frequency, one of the most commonly assigned codes. Scroll down in the Franklin ACMG Classification tab until you find the “Population Data” evidence category. You should see the PM2 code if this variant is rare: PM2 (Moderate Evidence of Pathogenicity) - “Absent from controls (or at extremely low frequency) in gnomAD Exome or Genome or Exome Sequencing Project” Click “See Details” so Franklin shows you the evidence used: gnomAD maximal non-founder subpopulations frequency: 0.0% gnomAD maximal founder subpopulations frequency: 0.0% Recommended PM2 frequency threshold for gene: 0.05% (this is gene-specific) Most frequent known pathogenic variant in gene: Lists examples with their frequencies Why does Franklin assign PM2 here? The variant is absent (frequency = 0.0%) in all gnomAD populations. The ACMG guidelines state that variants absent from large population databases can be assigned PM2, with the caveat that the frequency threshold must be appropriate for the gene’s disease mechanism and inheritance pattern. Step 5: Editing Evidence Strength Franklin’s automated assignments are starting points. Your role is to review each assigned code and decide whether you agree with both the code assignment and its strength. How to adjust evidence: Locate an assigned ACMG code (e.g., PM2 with the red badge) Click on the “Edit” section of the code to expand its detail view You’ll see two main controls: Met/Unmet toggle: Switches the code on or off, and determines whether the code contributes to the classification (ON) or not (OFF). Strength selector: Dropdown menu to adjust evidence weight. Some ACMG codes allow strength adjustments. For example, PM2 (Moderate) can potentially be adjusted to: Supporting (PP2-like): If frequency is at the threshold boundary Moderate (PM2): Standard assignment (Genoox’s suggestion) Strong (PS2-like): Generally not applicable for frequency evidence alone The “Preview” Feature: After you adjust any evidence code, Franklin displays a “Preview” section showing: Suggested classification: How the classification changes with your edit Visual scale: Updated position on the benign-to-pathogenic thermometer Point total: Recalculated evidence score This real-time feedback helps you understand how each piece of evidence contributes to the final classification. Step 6: Exploring Additional Evidence Categories Beyond population frequency, Franklin organises evidence into several other categories. Scroll through the Franklin ACMG Classification tab to explore: Effect on Protein (Functional Consequence): PVS1: Null variant (nonsense, frameshift, canonical splice site) in a gene where loss of function causes disease PM4: Protein length changes (in-frame deletions/insertions in non-repeat regions) Franklin automatically detects variant consequences using transcript annotations. For PVS1, it checks: Whether the gene is known to cause disease via loss of function Whether the variant truly disrupts protein function (not all nonsense variants do due to alternate transcripts or rescue mechanisms) Whether the variant affects a critical exon In Silico Predictions: PP3: Multiple computational algorithms predict deleterious effect BP4: Multiple computational algorithms predict benign effect Franklin aggregates predictions from SIFT, PolyPhen-2, REVEL, CADD, and others. The criterion is met when the majority of algorithms agree. You can expand this section to see individual tool scores. ⚠️ Important caveat: In silico predictions should not be used as standalone evidence. They’re supporting evidence that strengthens other findings but cannot drive classification alone (especially for VUS to Likely Pathogenic upgrades). Part 3: Uploading and Classifying Your Case VCF (45 minutes) Now that you’ve explored Franklin’s classification interface, you’ll upload your own VCF file and work through a complete variant interpretation workflow. Step 1: Download Your Case Data Before creating a case in Franklin, you need to download the VCF file from the JupyterHub environment. Procedure: Return to your JupyterHub session (the browser tab with the file browser) Navigate to the Module3/Hereditary directory Locate the file: hereditary_partial.hard-filtered.vcf.gz Right-click on the file and select “Download” Save it to a location you can easily access (e.g., your Desktop or Downloads folder)  Why this file? The hereditary_partial.hard-filtered.vcf.gz file contains variant calls from a hereditary cancer case, already filtered to remove low-quality calls. This is a realistic representation of what you’d receive from a sequencing core facility or bioinformatics pipeline. The “partial” designation means we’ve subset to a manageable number of variants for this exercise. Step 2: Create a New Case in Franklin Return to your Franklin browser tab and navigate to the Search page (the default landing screen). Creating the case: In the centre of the page, you’ll see three case creation options. Click on “VCF” (the third option) Franklin opens the case creation form with several sections to complete Case Configuration Form: Case Type Selection: Select “Inherited Disease” (this applies germline ACMG classification rules) Choose “Single Case” (we’re analysing one individual, not a family trio or cohort) Case Information Section: You’ll see several fields to fill in. Here’s what to enter: Case Name (required): Choose something you’ll recognise as your practice exercise Examples: Module3_Exercise, Hereditary_Workshop, Your_Name_VCF_Lab Avoid generic names like “Test” or “Case1” if multiple people are using the same account Patient ID (optional): You can leave this blank or use a placeholder like Patient_001 Gender (optional): Select any option (this affects X-linked variant interpretation, but we won’t focus on that today) Date of Birth (optional): Can be left blank for this exercise Referring Physician (optional): Can be left blank Clinical Indication (optional): If you want to practise, you could enter “Hereditary cancer syndrome” or similar ⚠️ Important: The case name is the only field that truly matters for this exercise. Everything else is optional metadata that would be relevant in a clinical setting. Don’t spend time overthinking these fields, since you can always modify case details later. VCF File Upload: Scroll to the bottom of the case creation form You’ll see a file upload section labelled “VCF File” Click “Choose File” or drag-and-drop your downloaded hereditary_partial.hard-filtered.vcf.gz file Franklin accepts gzipped VCF files directly (no need to decompress) Reference VCF &amp; Genome Verification: Before clicking “Create Case,” verify the reference genome selection is hg19 (GRCh37) This should match the genome build used in the variant calling pipeline Check the VCF file format version (should be VCFv4.2) zgrep &quot;^##fileformat&quot; hereditary_partial.hard-filtered.vcf.gz Check the reference genome or contig naming (should mention hg19 or GRCh37) zgrep &quot;^##reference&quot; hereditary_partial.hard-filtered.vcf.gz Create the Case: - Click “Create Case” at the bottom of the form - Franklin will upload your VCF and begin processing (this typically takes 30-60 seconds) ⏸️ Break Time: While your VCF uploads and processes, take a short break. When you return, we’ll navigate to your case and begin the variant classification workflow. Part 4: Variant interpretation Step 3: Navigating to Your Case Locate your newly created case in Franklin. Procedure: Click on the “My Cases” tab in the top navigation bar You should see your case listed with the name you chose (e.g., Module3_Exercise_Germline) The case status will show as “In Progress” with a count of variants detected Click on the case name to open the variant workbench Franklin will display the variant classification workspace you saw in Part 2, but now populated with variants from your uploaded VCF file. Step 4: Reviewing BRCA2 Variants in Your Case Your VCF file contains two clinically relevant BRCA2 variants that we’ll focus on for this exercise. In your Workbench section, you should see: BRCA2:c.1813dup (also written as c.1813dupA or p.Ile605Asnfs*4) BRCA2:c.1310_1313del (also written as c.1310_1313delAAGA or p.Glu437Valfs*22)  Why these variants? Both are frameshift variants in BRCA2, a gene associated with hereditary breast and ovarian cancer syndrome. Frameshift variants typically trigger PVS1 (very strong evidence of pathogenicity) because they create premature stop codons and result in loss of protein function. Click on BRCA2:c.1813dup to open its classification workspace (Use the arrow head to the right of the variant bar). Take note of: Automatically assigned ACMG codes: Franklin should have assigned several codes based on the variant’s properties Suggested classification: Check whether Franklin suggests this as Pathogenic, Likely Pathogenic, or VUS Population frequency: Review the PM2 evidence if assigned Functional consequence: Look for PVS1 assignment (null variant in a loss-of-function gene) Repeat this process for BRCA2:c.1310_1313del and compare the evidence profiles. Do both variants receive similar classifications? Are there any differences in the evidence codes assigned? Step 5: Adding Patient Phenotype Information Clinical variant interpretation is significantly enhanced when you provide patient phenotype data. Franklin uses Human Phenotype Ontology (HPO) terms to prioritize variants in genes associated with the patient’s clinical features. Understanding the Phenotype JSON File: Your course materials include a file called phenotips_data.json located in CourseData/Module3/Hereditary/. This file was exported from a clinical phenotyping system and contains structured patient information. Extracting Relevant HPO Terms: Open the phenotips_data.json file in a text editor (or view it in the Jupyter notebook) less phenotips_data.json Look for the “features” section, which contains a list of phenotypic observations For each feature, check the “observed” field: \"observed\": \"yes\" → Include this HPO term \"observed\": \"no\" → Do NOT include this HPO term Expected HPO Terms to Extract: From the JSON file, you should find the following observed phenotypes: HP:0100615 - Ovarian neoplasm HP:0032317 - Family history of cancer HP:0012378 - Fatigue HP:0000819 - Diabetes mellitus You can also extact relevant phenotype terms using a quick one-line command: jq -r &#39;.features[] | select(.observed==&quot;yes&quot;) | &quot;\\(.id) - \\(.label)&quot;&#39; phenotips_data.json Here is a breakdown of the code: jq : A small tool for reading and filtering JSON files. -r :Raw output (no quotes around strings). .features[] : Start at the top of the JSON, then iterate over each item in the features list. select(.observed==&quot;yes&quot;) : Keep only features where &quot;observed&quot; equals &quot;yes&quot;. &quot;\\(.id) - \\(.label)&quot; : Print the HPO ID and its label on one line. phenotips_data.json : The JSON file you’re reading. ⚠️ Important: Do NOT add HP:0001737 (Pancreatic cysts). This has \"observed\": \"no\" in the JSON file, meaning the patient does NOT have this feature. Adding Phenotypes to Your Franklin Case: In your case view, locate the “Phenotype” section (typically in the right sidebar or case details panel) Click “Add/Remove Phenotype” or the “+” icon In the search box, enter either the HPO ID (e.g., HP:0100615) or the phenotype name (e.g., Ovarian neoplasm) Franklin will provide autocomplete suggestions - select the correct term The term will be added to your phenotype list Repeat for all four observed phenotypes listed above Adding Patient Demographics: While in the case details, you can also add: Age: 45 years old Sex: Female How Phenotypes Affect Variant Prioritization: After adding the HPO terms, return to your variant list. You should notice: The “Phenotype Match” column will show scores indicating how well each variant’s gene matches the patient’s clinical features Franklin may automatically filter or deprioritize variants in genes unrelated to the patient’s presentation. Step 6: Exploring Case-Level Quality Control and Variant Filtering Before diving into detailed variant interpretation, it’s valuable to explore the additional information Franklin provides about your case. Understanding these features will help you assess data quality and efficiently filter variants. Quality Control Tab: Navigate to the Quality Control tab in your case view. You’ll likely see several warnings or quality metrics displayed. ℹ️ Important context: Because you’re working with a partial VCF file (subset of variants from the complete case), some quality calculations may not be accurate. However, it’s still important to understand what quality parameters Franklin monitors in a complete clinical workflow. Key Quality Metrics to Review: 1. Sex Detection and Ploidy Parameters (not apploicable to this case) Franklin analyzes the ratio of X and Y chromosome variants to determine biological sex. This is critical for: Verifying sample identity (does the detected sex match the clinical record?) Interpreting X-linked variants correctly Detecting sample mix-ups or contamination For example, if the case metadata says “Female” but Franklin detects XY chromosomes, this indicates a potential sample labelling error. 2. Average Variant Depth This metric shows the mean read depth across all called variants. For your case, this should be approximately 140× coverage, which is typical for exome sequencing. Why this matters: Low average depth (&lt;20×): May miss variants, especially heterozygous ones Expected range (100-150×): Appropriate for clinical exome analysis Very high depth (&gt;500×): Typical for targeted panels, not relevant here 3. Other Quality Indicators Franklin may also display: Variant-to-reference ratio: Helps detect contamination Transition/transversion ratio: Should be ~2.0-2.1 for exomes (genome-wide quality metric) Coverage uniformity: Percentage of target regions meeting minimum depth thresholds ⚠️ Clinical practice note: In a real diagnostic workflow, you would carefully review all quality warnings before proceeding with interpretation. Any failures would require troubleshooting or repeat analysis. Exploring the Variant Tab: Now navigate to the Variants tab on the left side of your case view. This is where you’ll see a significant difference from the Workbench. Workbench vs. Variants Tab: Workbench: Shows only variants likely to be clinically reportable (typically classified as Pathogenic, Likely Pathogenic, or clinically relevant VUS) Variants Tab: Displays all variants present in your VCF file, including benign polymorphisms and low-quality calls that passed initial filtering Understanding the Filter Panel: On the left side of the Variants tab, Franklin provides comprehensive filtering options. These filters allow you to narrow down variants based on various criteria: 1. Phenotype Filters If you’ve added HPO terms (as you did in Step 5), you’ll see a Phenotypes filter section at the top. You can: Filter to show only variants in genes associated with your patient’s phenotypes This dramatically reduces the variant list to clinically relevant candidates 2. Franklin Classification Filter One of the most useful filters for clinical workflows: Click on the “Franklin Classification” filter Select “Pathogenic” to see only variants Franklin suggests as Pathogenic You should see your two BRCA2 frameshift variants appear Try toggling between different classification categories (Pathogenic, Likely Pathogenic, VUS, Likely Benign, Benign) to see how the variant list changes. 3. Additional VCF-Based Filters Franklin exposes all the information encoded in your VCF file as filterable parameters: Variant Type: SNV, insertion, deletion, complex Genomic Region: Exonic, intronic, splice site, UTR Gene: Filter by specific gene names Quality Metrics: Depth, quality score, allele fraction Population Frequency: Filter by gnomAD frequency thresholds Consequence: Missense, nonsense, frameshift, synonymous Inheritance Pattern: Match to disease inheritance models Practical Exercise: Try the following filtering combinations to see how they affect your variant list: Filter by Franklin Classification = “Pathogenic” Result: Should show your two BRCA2 frameshift variants Filter by Phenotype = “Ovarian neoplasm” Result: Should prioritise variants in genes associated with ovarian cancer Filter by Variant Type = “Deletion” Result: Shows only deletion variants, including BRCA2:c.1310_1313del Step 7: Deep Dive into Variant Classification: BRCA2:c.1813dup Now that you’ve explored the case-level features, let’s perform an interpretation of one of your BRCA2 variants. We’ll use BRCA2:c.1813dup as our example, though you can apply the same process to any variant in your case. Opening the Detailed Variant View: Navigate back to your Workbench tab Locate BRCA2:c.1813dup in your variant list Click anywhere on the variant bar EXCEPT the arrow icon on the right. It should open the detailed variant assessment view Franklin will load a variant detail page with multiple tabs and information panels (similar to what you saw with the initial example). Reviewing the Franklin ACMG Classification The first tab you’ll see is Franklin ACMG Classification. This is where you evaluate the automatically assigned evidence codes and understand how they contribute to the classification. The thermometer position is calculated based on the cumulative strength of evidence codes assigned. For BRCA2:c.1813dup (a frameshift variant), you should see the marker positioned in the Pathogenic range. ClinGen Gene-Specific Guidelines: Below the thermometer, you’ll notice a Franklin Highlight section stating: “ClinGen has gene-specific guidelines for BRCA2”. Click on this link to access the ClinGen Expert Panel specifications for BRCA2. This is important because: Different genes have different tolerances for variation Some ACMG codes may not apply to certain genes or disease mechanisms Evidence strength may be adjusted based on gene-disease relationships Specific code combinations may be recommended or cautioned against What you’ll find in the ClinGen BRCA2 guidelines: When you click through to ClinGen, scroll down to review: Gene-disease relationship: BRCA2 and hereditary breast-ovarian cancer (HBOC) Recommended modifications to ACMG criteria: Which codes are particularly informative for BRCA2 Any codes that should NOT be used Strength adjustments for specific evidence types Code-by-code specifications: Detailed guidance on applying each ACMG criterion to BRCA2 variants ⚠️ When to consult gene-specific guidelines: These are particularly valuable when you have borderline classifications (e.g., a VUS that’s close to Likely Pathogenic, or a Likely Pathogenic that might be upgraded to Pathogenic). The expert panel guidance can provide the justification needed for appropriate classification. Reviewing Assigned ACMG Codes: Scroll through the Franklin ACMG Classification tab and review each assigned code. For BRCA2:c.1813dup, you should see codes like: PVS1: Null variant in a gene where loss of function is a known disease mechanism PM2: Absent from population databases (or extremely rare) PP3: Multiple computational predictions suggest a deleterious effect (if applicable to frameshift context) Take time to expand each code and review the supporting evidence Franklin used to make the assignment. Exploring the Variant Assessment Tab Click on the Variant Assessment tab. This tab contains multiple information panels organized on the left sidebar. Each panel provides different types of evidence to support your interpretation. 1. Links The first section provides quick access to external databases: rs80359306: dbSNP reference SNP ID (click to view in NCBI dbSNP) UCSC: Opens the UCSC Genome Browser at this variant’s genomic location gnomAD: Direct link to this variant’s entry in the gnomAD database These links allow you to verify Franklin’s annotations and explore additional information in the source databases. 2. Region Viewer This interactive panel shows how other variants in the same genomic region are classified in ClinVar. Features to explore: Genomic context: Visual representation of the BRCA2 gene region surrounding your variant ClinVar variant overlay: Red and green markers showing pathogenic and benign variants respectively Filter buttons: LOF (Loss of Function): Shows nonsense, frameshift, and splice variants Missense: Shows amino acid substitution variants Non-coding and Synonymous: Shows variants outside coding regions or with no amino acid change Using the Region Viewer: Click LOF to filter the view to show only loss-of-function variants like yours Observe the distribution of pathogenic (red) vs. benign (green) markers Click “Select Track” on the right to add additional annotation tracks (e.g., protein domains, conservation scores)  Interpretation insight: If you see many pathogenic LOF variants clustered in this region of BRCA2, it strengthens the evidence that null variants in this part of the gene are disease-causing. Conversely, if you saw many benign LOF variants, it might suggest this region tolerates protein truncation (which is NOT the case for BRCA2). 3. Franklin Community Frequency This section shows how many times this variant has been observed in the Franklin user community.bFor BRCA2:c.1813dup, you should see: “80 cases - Very Rare variant in Franklin community”. This indicates that while the variant has been observed, it remains rare across Franklin’s user base. ℹ️ Context: Franklin Community data is distinct from gnomAD population data. Franklin users are typically clinical cases undergoing genetic testing, whereas gnomAD represents general population controls. A variant can be rare in gnomAD but relatively more common in Franklin if it’s a recurrent pathogenic mutation. 4. Variant Priority This metric assesses how likely this variant is to be clinically reportable based on: Classification (Pathogenic/Likely Pathogenic = high priority) Gene-disease association strength Phenotype match (if you added HPO terms) For your BRCA2 variant with the ovarian neoplasm phenotype, this should show high priority. 5. Confidence This section displays quality metrics from the VCF file to assess whether the variant call is reliable or potentially an artifact. Key metrics to review: Allele Fraction (AF): For a germline heterozygous variant, expect ~50% For BRCA2:c.1813dup, you should see approximately 56% AF This is close to the expected 50%, confirming this is likely a true heterozygous variant Deviations from 50% could indicate: Copy number changes affecting the region Subclonal mosaicism Technical artifacts or contamination Read Depth (DP): Number of reads covering this position Higher depth = more confidence in the call For exome data, expect 100-200× depth at well-covered exons Genotype Quality (GQ): Phred-scaled confidence in the genotype call GQ ≥20 is typically considered high confidence (99% certainty) ✅ Quality checkpoint: The ~56% allele fraction with high depth confirms this is a real heterozygous variant, not a sequencing artifact. You can proceed with interpretation confidently. 6. Clinical Evidence This section links your variant to existing clinical interpretations in ClinVar. Click to expand and review: ClinVar classification: How other labs have classified this variant Submission count: How many independent labs have reported this variant Star rating: ClinVar’s confidence level based on agreement across submitters Review status: Whether the classification has expert panel review For BRCA2:c.1813dup, you should see existing Pathogenic classifications from multiple submitters, providing strong supporting evidence. 7. Predictions This section displays in silico algorithm predictions about the variant’s functional impact. Important context for frameshift variants: Since BRCA2:c.1813dup is a frameshift (not a missense variant), most amino acid substitution predictors (SIFT, PolyPhen-2, etc.) are not applicable. Instead, you’ll primarily see: SpliceAI: Predicts whether the variant affects RNA splicing For frameshifts, splicing disruption can compound the truncation effect Check if SpliceAI predicts splice donor/acceptor gain or loss  Missense vs. Frameshift predictions: If you were analyzing a missense variant instead, this section would show SIFT, PolyPhen-2, REVEL, CADD, and other algorithms that assess amino acid substitution consequences. For LOF variants, these tools are less informative, so Franklin appropriately shows only splice-relevant predictions. 8. Internal Frequency This metric would show how frequently the variant appears in your own cohort or laboratory’s previous cases. For this workshop exercise with a single case, this field is not applicable. In a clinical laboratory setting with hundreds of cases, this helps identify: Recurrent pathogenic mutations Technical artifacts that appear repeatedly across runs Population-specific variants in your patient population 9. Population Frequencies This critical section queries multiple population databases to determine variant rarity. Databases queried: gnomAD (primary): Genome Aggregation Database with &gt;140,000 individuals ExAC: Exome Aggregation Consortium (now incorporated into gnomAD) 1000 Genomes Project: International reference dataset ESP (Exome Sequencing Project): NHLBI dataset Population-specific databases: TopMed, UK10K, etc. What to look for: For BRCA2:c.1813dup, you should see: gnomAD frequency: 0.0% (absent in all populations) This supports the PM2 ACMG code assignment (absent/rare in controls) Critical interpretation rule: Variants with &gt;5% frequency in any population are typically classified as Benign (BA1 code) Variants with &gt;1% frequency require careful consideration (BS1 code may apply) Variants absent or extremely rare (&lt;0.01%) support pathogenicity (PM2 code) 10. Transcripts and References This section provides HGVS nomenclature for the variant across different transcript isoforms and reference sequences. Information provided: HGVS coding (c.): c.1813dup (coding DNA level) HGVS protein (p.): p.Ile605Asnfs*4 (protein level - indicates frameshift starting at isoleucine 605) Alternate nomenclature: How this variant may be referenced in older publications Effect across transcripts: Whether the variant affects all BRCA2 isoforms or only specific ones When searching literature for prior reports of this variant, authors may use different transcript references. This section helps you identify all possible names for the same variant. 11. Suspected Compound Variants This section identifies other variants in the same gene (BRCA2) that appear in your case. Clinical relevance: For autosomal recessive conditions: Two variants in the same gene (one on each chromosome) are required for disease For BRCA2 (dominant condition): This is less relevant, but can still be informative Phasing requirement: To determine if variants are in cis (same chromosome) or trans (different chromosomes), you need parental samples 12. Gene Coverage This section would display read depth coverage across the BRCA2 gene if a BAM file were uploaded. This section will show “No coverage data available.” However, you can still visualize coverage using the BAM file from your Jupyter Notebook environment. Alternative visualization with IGV (optional): Download and install IGV (Integrative Genomics Viewer) from igv.org Download the hereditary_partial.bam file from your Module3/Hereditary/ directory. Once downloaded, load .bam into IGV (make sure that the .bai file is in the same folder Navigate to the BRCA2 gene (chr13:32,907,420 in hg19/GRCh37) Zoom into the region around c.1813dup Observe: Read depth across the region Individual reads supporting the reference vs. alternate allele Read quality and mapping quality Presence of any alignment artifacts This manual inspection can help you identify potential technical issues that automated callers might miss. 13. Additional Tools and Resources Scroll to the bottom of the Variant Assessment tab to find links to: Sequence Browser: Explore the DNA sequence context around the variant Exploring Associated Conditions Navigate to the Associated Conditions tab to see how your variant connects to clinical phenotypes. This view integrates three key elements: the variant, the gene (BRCA2), and the patient phenotypes you added earlier. Top Section: Primary Condition Match At the top of this view, you’ll see the primary conditions associated with BRCA2 alongside the phenotypes you added to your case (ovarian neoplasm, family history of cancer, fatigue, diabetes mellitus). Key features to observe: Association strength gauge: On the left side, you’ll see a visual gauge indicating the strength of association between BRCA2 and your observed phenotypes The gauge position reflects how strongly BRCA2 variants are linked to these clinical features in the literature For ovarian neoplasm and family history of cancer, expect a strong association given BRCA2’s well-established role in hereditary breast-ovarian cancer syndrome Scrolling Through Associated Conditions As you scroll down, Franklin displays other conditions associated with BRCA2 variants: Condition descriptions: Brief summaries of each disease phenotype Phenotype matching indicators: Franklin highlights which of your patient’s phenotypes match each condition Green checkmarks or highlighted terms indicate perfect matches Grey or absent terms indicate phenotypes not observed in your patient Expanding Condition Details Click on any condition to see more specific information: Click the condition name to expand the detail panel Click the arrow icon on the right side to access condition-specific information You’ll see: Complete HPO term list: All phenotypes associated with this specific condition or sub-specification Matched terms highlighted in green: HPO terms from the condition that match your patient’s phenotypes Inheritance pattern: Whether the condition follows autosomal dominant, recessive, or X-linked inheritance Age of onset: Typical age range when symptoms appear  Clinical value: This feature helps you assess whether your patient’s presentation matches the expected phenotype for a BRCA2-related condition. Strong phenotype matches increase confidence that a variant is clinically relevant to the patient’s case. Using the Publications Tab Click on the Publications tab to access Franklin’s curated scientific literature relevant to your variant and case. This is one of Franklin’s most powerful features for clinical variant interpretation. Why this matters: Variant interpretation often requires literature review to find: Prior case reports describing this exact variant Functional studies demonstrating pathogenic mechanism Clinical outcome data supporting actionability Genotype-phenotype correlation studies Franklin’s algorithm automatically curates and ranks publications by relevance, saving you significant time compared to manual PubMed searches. Understanding the Scope Filter When you first open the Publications tab, Franklin displays papers by default at the whole gene level. Use the Scope dropdown to refine your search: Variant: Papers specifically mentioning your exact variant (BRCA2:c.1813dup) For this variant, expect approximately 53 articles Amino Acid: Papers discussing variants at this specific amino acid position Domain: Papers about the protein domain containing your variant Whole Gene: All BRCA2-related publications (thousands of papers) Recommended workflow: Start with Scope = Variant to see if this exact variant has been reported If no variant-specific papers exist, expand to Amino Acid or Domain Use Whole Gene scope for background reading on BRCA2 function and disease associations Reviewing Individual Publications Each publication entry displays: Title: Full publication title Authors and journal: Publication details PMID (PubMed ID): Click to open the full record in PubMed Abstract preview: Click the publication to expand and read the abstract within Franklin Practical filtering strategy: As you review a few articles for BRCA2:c.1813dup, you can prioritize based on: Case reports: Skim abstracts to verify the variant and patient phenotype match your case Functional studies: Read in detail if they provide experimental evidence of pathogenicity Population studies: Note allele frequencies in specific populations Review articles: Use for background understanding but don’t cite as primary evidence  For your report: Select 2-3 key publications that provide the strongest evidence for your variant’s pathogenicity. You might include: A case report demonstrating clinical impact A functional study showing loss of protein function A large cohort study confirming recurrence in affected families Gene Assessment Tab The final core section for variant interpretation is the Gene Assessment tab. This provides gene-level context that complements your variant-specific evidence. Why review gene-level information? When reporting a variant clinically, you typically include: Variant-specific evidence (what you’ve reviewed so far) Gene-level summary (function, disease mechanism, constraint metrics) This dual approach helps clinicians understand both the specific variant and the broader biological context. 1. Gene Summary At the top of the Gene Assessment tab, you’ll see: Function description: Summarizes BRCA2’s biological role (typically sourced from RefSeq or NCBI Gene) For BRCA2, this will describe its role in DNA repair, homologous recombination, and tumour suppression Disease mechanism: How loss of BRCA2 function leads to cancer predisposition 2. Curated Variations Distribution This section shows the breakdown of all reported BRCA2 variants by type and classification. Variant type categories: LOF (Loss of Function): Nonsense, frameshift, canonical splice variants Missense: Amino acid substitutions In-frame Indels: Insertions or deletions that don’t shift the reading frame Non-coding: Intronic, UTR, or intergenic variants Synonymous: Silent variants that don’t change amino acids Classification breakdown: For each variant type, you’ll see counts for: Pathogenic (red bars) VUS (yellow bars) Benign (green bars) Interpreting the distribution for BRCA2: You should observe that LOF variants in BRCA2 have a significantly higher proportion classified as Pathogenic compared to other variant types. This confirms that: BRCA2 is highly sensitive to loss of function Null variants are a well-established disease mechanism Your frameshift variant (BRCA2:c.1813dup) falls into a variant class with strong prior evidence of pathogenicity  Clinical interpretation tip: If you were analyzing a gene where LOF variants were predominantly benign or VUS, it would suggest the gene tolerates loss of function, and you’d need stronger evidence to classify a frameshift as pathogenic. The BRCA2 distribution strongly supports PVS1 code assignment for your variant. 3. Gene Pathogenicity Metrics Scroll down to the Gene Pathogenicity section to review constraint and dosage sensitivity data from multiple sources. ClinGen Dosage Sensitivity: Haploinsufficiency: Does losing one functional copy of the gene cause disease? For BRCA2, this should show “Sufficient evidence for haploinsufficiency” This confirms that heterozygous loss-of-function variants (like yours) are pathogenic Triplosensitivity: Does gene duplication cause disease? Less relevant for BRCA2, but important for other genes (e.g., dosage-sensitive developmental genes) ClinVar Pathogenicity Summary: Franklin displays aggregated ClinVar data showing: Total number of BRCA2 variants in ClinVar Breakdown by classification (Pathogenic, Likely Pathogenic, VUS, Benign) Most common variant types reported gnomAD Constraint Metrics: This section shows statistical evidence for negative selection against variants in BRCA2. Key metrics explained: pLI (probability of Loss of Intolerance): Score from 0-1 indicating how intolerant a gene is to loss-of-function variants pLI &gt; 0.9 suggests the gene is highly intolerant (LOF variants are under strong negative selection) For BRCA2, expect a moderate to high pLI score Z-score: Compares observed vs. expected missense variants Positive Z-score = fewer variants observed than expected (intolerant to variation) Negative Z-score = more variants than expected (tolerant to variation) Observed/Expected (o/e) ratio: Direct calculation of variant burden o/e &lt; 0.35 suggests strong constraint o/e close to 1.0 suggests no constraint If gnomAD shows significantly fewer LOF variants observed than expected in the general population, it indicates strong selection pressure against loss of BRCA2 function. This supports: The gene is critical for human health LOF variants are likely to be pathogenic Your frameshift variant fits the expected disease mechanism You can click the gnomAD link to explore detailed constraint metrics directly in the gnomAD browser. 4. Gene Expression Data (GTEx) The final section provides tissue expression information from GTEx (Genotype-Tissue Expression project). Here you’ll see Tissue expression heatmap: Shows in which tissues BRCA2 is expressed For BRCA2, you should see broad expression across many tissues, including ovarian tissue (relevant to your patient’s phenotype) Expression level: Relative expression intensity (darker colours = higher expression) Transcript-level expression: Some exons may be preferentially expressed in certain tissues Why this is clinically relevant: For BRCA2, you already know the gene is relevant to ovarian tissue because of its well-established role in hereditary cancer. However, for less well-characterized genes, confirming tissue expression is critical: If your patient has a kidney disorder, but the gene is not expressed in kidney tissue, the variant is unlikely to be causative If multiple transcripts exist, confirming which transcript is expressed in the relevant tissue helps determine if your variant affects the functional isoform Exploring GTEx in detail (optional): Click the GTEx Portal link to access: Quantitative expression levels across 54 tissues Exon-level expression patterns Transcript isoform usage in different tissues eQTL data (if common variants affect gene expression) 5. GWAS Traits (optional) At the bottom of the Gene Assessment tab, you may see GWAS (Genome-Wide Association Study) Traits linking BRCA2 to population-level traits or disease risks. This information is supplementary and not required for clinical variant interpretation. ✅ You’ve now reviewed all core sections of Franklin’s variant interpretation workspace. You’ve examined: Automated ACMG classification and evidence codes Population frequency data Computational predictions Clinical evidence from ClinVar Associated phenotypes and conditions Scientific literature Gene-level constraint and expression data This comprehensive review equips you with the evidence needed to confidently classify and report BRCA2:c.1813dup. Step 8: Finalizing Your Classification and Creating a Clinical Report Now that you’ve reviewed all the evidence, it’s time to formalize your classification and generate a report that can be shared with clinicians and other team members. Locating the Classification Controls In the detailed variant view where you’ve been working, look at the top right corner of the page. You’ll see several action buttons: Remove from Workbench: Removes the variant from your prioritized list (don’t use this for BRCA2:c.1813dup, as it’s clearly clinically relevant) Add to Report: Marks this variant as reportable for the final clinical report Classify Variant: Opens the classification form (this is what you’ll click) 1. Click “Classify Variant” Franklin will open a classification form with several sections to complete. 2. Select Your Classification The first dropdown asks you to choose a classification tier based on your interpretation: Pathogenic Likely Pathogenic VUS (Variant of Uncertain Significance) Likely Benign Benign For BRCA2:c.1813dup, based on the evidence you’ve reviewed (PVS1 + PM2 + supporting evidence), this should be classified as Pathogenic. 3. Assign Associated Condition and Inheritance In the next section, you’ll specify: Associated condition: Select the most appropriate disease phenotype for this variant For BRCA2:c.1813dup, choose “Hereditary breast and ovarian cancer syndrome” or similar terminology that Franklin provides Inheritance pattern: Select the mode of inheritance For BRCA2, select “Autosomal dominant” This information helps clinicians understand the genetic counselling implications and family screening recommendations. 4. Write Your Interpretation Summary This is one of the most critical sections. You’ll provide a brief narrative explaining your classification decision. Key principles for writing interpretation text: Use plain language: Avoid ACMG jargon that clinicians may not understand ❌ Don’t say: “This variant meets PM2, PVS1, and PP3 criteria” ✅ Do say: “This variant is absent from population databases, creates a premature stop codon in a gene where loss of function causes disease, and is predicted to be deleterious by multiple computational algorithms” Be concise: Aim for 3-5 sentences that summarize the key evidence Include clinical context: Reference the patient’s phenotype if relevant Example interpretation for BRCA2:c.1813dup: This variant is a frameshift deletion resulting in a premature stop codon at amino acid position 605. The variant is absent from large population databases (gnomAD, ExAC), indicating it is not a common benign polymorphism. BRCA2 is a well-established tumor suppressor gene where loss-of-function variants cause hereditary breast and ovarian cancer syndrome through an autosomal dominant mechanism. This variant has been reported multiple times in ClinVar with consistent Pathogenic classifications from multiple clinical laboratories. The patient’s clinical presentation of ovarian neoplasm and strong family history of cancer is consistent with BRCA2-associated disease. 5. Add Supporting References (Optional) If you identified particularly strong publications during your literature review, you can add them here: Enter the PMID (PubMed ID) of relevant papers Include papers that provide: Functional evidence of loss of protein function Case reports with similar phenotypes Population studies confirming pathogenicity  You don’t need to cite every paper you reviewed. Select 2-3 key publications that provide the strongest supporting evidence. Prioritize functional studies and expert panel consensus statements over individual case reports. 6. Review and Edit the Classification Summary Franklin provides a structured summary based on the information you’ve entered. You can: Accept the default summary: If Franklin’s auto-generated text is accurate Edit the summary: Modify any sections that need clarification or additional detail Add gene-level context: Include information about BRCA2 function and disease mechanism if not already present 7. Assign ACMG Evidence Codes The final section of the form allows you to document which ACMG criteria support your classification explicitly. Click the checkbox or toggle next to each code to include it in your formal classification. ⚠️ Important: Only assign codes where you have verified the supporting evidence. Don’t accept all of Franklin’s automated suggestions if you identified reasons to remove or downgrade any codes during your review. 8. Submit Your Classification Once you’ve completed all sections of the form, click the “Submit” button in the top right corner. You’ll see two submission options: Submit as Draft: Saves your classification to the report repository but marks it as requiring review Use this option for this workshop exercise. In a clinical setting, this allows supervisors or genetic counsellors to review your work before finalization Submit as Final: Locks the classification and generates a final report Only use this if you have sign-off authority (typically molecular geneticists or licensed genetic counsellors) Understanding the Collaborative Workflow: The variant interpretation process you’ve just completed mirrors real-world clinical laboratory workflows: 1. Genome Analysts / Bioinformaticians: Perform initial variant calling and quality control Run automated annotation and filtering Generate prioritized variant lists Assign preliminary ACMG codes based on computational evidence. 2. Genetic Counsellors: Deep dive into patient phenotype and clinical history Assess gene-disease associations and inheritance patterns Review family history and segregation evidence Ensure phenotype matches expected disease presentation. 3. Molecular Geneticists / Laboratory Directors: Final review of all evidence Sign off on classification decisions Ensure compliance with ACMG/AMP guidelines and regulatory standards Authorize release of report to ordering physician Part 5: Extension Exercise (Optional) If you have additional time, try classifying the second BRCA2 variant in your case: **BRCA2:c.1310_1313del (p.Glu437Valfs*22)** Click on this variant in your Workbench Work through the same systematic review process: Check Franklin ACMG Classification codes Review Variant Assessment evidence Check Associated Conditions and Publications Review Gene Assessment information Complete the classification form Compare your classification between the two BRCA2 variants Discussion questions: Do both variants receive the same classification? Are there any differences in the supporting evidence? Which variant would you report first if you could only report one? How would you counsel this patient based on both findings? Summary This lab shows the transition from computational variant calling to clinical interpretation. You’ve seen how: Automation handles data processing but requires human curation: Franklin’s algorithms assign ~60-70% of ACMG evidence automatically, but you must verify assignments and add clinical context Population frequency is powerful but not sufficient: Rarity alone doesn’t prove pathogenicity; you need functional consequence, gene constraint, and clinical evidence Phenotype matching strengthens interpretation: HPO terms help prioritize variants and confirm gene-disease associations Literature curation saves time: Franklin’s automated literature ranking is one of its most valuable features, but you must still evaluate publications Classification is a team effort: Real diagnostic labs involve multiple specialists with complementary expertise "],["module-4-lab---case-3-stk11-deletion-in-peutz-jeghers-syndrome.html", "Module 4 Lab - Case 3: STK11 Deletion in Peutz-Jeghers Syndrome Learning Objectives Background Section 1: Installing the Integrative Genomics Viewer Section 2: Acquiring Case Study Data Files Section 3: Loading Data into IGV and Navigating to STK11 Section 4: Quality Assessment of the Suspected Deletion Section 5: Clinical Interpretation of Copy Number Loss Using ClinGen Guidelines Step 6: Calculating the Final Classification Step 7: Clinical Recommendations and Validation References", " Module 4 Lab - Case 3: STK11 Deletion in Peutz-Jeghers Syndrome Summary: The laboratory session focuses on assessing the STK11 deletion in Peutz-Jeghers syndrome, emphasizing systematic quality evaluation of copy number variants (CNVs) using IGV. Participants will learn to classify CNVs based on genomic content and patient phenotype, integrate bioinformatic evidence, and determine variant pathogenicity. The session covers the clinical implications of Peutz-Jeghers syndrome, the role of the STK11 gene, and the rationale for manual review of variant calls. Key steps include installing IGV, acquiring case study data, loading data into IGV, and performing quality assessments to differentiate true deletions from technical artifacts. Text: Module 4 Lab - Case Study on STK11 Deletion in Peutz-Jeghers Syndrome Learning Objectives By the end of this laboratory session, you will be able to: Perform systematic quality assessment of copy number variants (CNVs) in IGV by evaluating coverage patterns, mapping quality, breakpoint definition, and overlap with known technical confounders Apply the ClinGen dosage sensitivity framework to classify copy number variants based on genomic content, haploinsufficiency evidence, and patient phenotype concordance Integrate bioinformatic evidence with clinical context to determine variant pathogenicity in the context of hereditary cancer syndromes Background Peutz-Jeghers Syndrome Peutz-Jeghers syndrome (PJS) is an autosomal dominant hereditary cancer predisposition syndrome caused by pathogenic variants in the STK11 gene.[1] In autosomal dominant inheritance, a single mutated allele is sufficient to cause disease manifestation. Affected individuals have a 50% probability of transmitting the variant to each offspring. The clinical presentation of PJS includes: Mucocutaneous hyperpigmentation (melanotic macules on the lips, buccal mucosa, and perioral region) Multiple hamartomatous polyps throughout the gastrointestinal tract Elevated cancer risks: gastrointestinal malignancies (40-60%), breast cancer (45-50%), ovarian cancer (20%)[1] While the hamartomatous polyps themselves are typically benign, they frequently cause complications, including intussusception and bowel obstruction.[[2]] The substantially elevated lifetime cancer risks require intensive surveillance throughout the patient’s life. The STK11 Gene and Disease Mechanism STK11 (serine/threonine kinase 11), also known as LKB1, is located at chromosomal position 19p13.3. This tumour suppressor gene encodes a master kinase that regulates cellular polarity, energy metabolism, and growth control through the AMPK signaling pathway.[3] PJS operates through a haploinsufficiency mechanism. Loss of one functional STK11 allele results in insufficient kinase activity to maintain normal cellular homeostasis.[4] The molecular spectrum of pathogenic variants includes point mutations (approximately 52% of cases) and large genomic deletions (approximately 15% of cases).[5] Exon 7 of STK11 is particularly significant from a clinical perspective. This exon encodes part of the catalytic kinase domain, and mutations affecting this region have been frequently reported in PJS patients.[4] Rationale for Manual Review Clinical genomic analysis frequently encounters variant calls that require expert human interpretation beyond automated pipeline classification. This case presents a deletion call affecting STK11 exons 6 and 7 with characteristics that need to be evaluated. The variant caller assigned a quality score of 33, marginally above the laboratory threshold of 30 for reporting. Coverage analysis reveals the following pattern: Exon 6: Patient coverage approximately 45X; control samples 107-164X Exon 7: Patient coverage approximately 90X; control samples 152-205X This region presents known technical challenges. The sequence context is GC-rich (high guanine and cytosine content), which introduces systematic bias in PCR amplification and sequencing. The laboratory has documented false-positive copy number variant calls in this region previously.  Your objective is to determine whether this represents a true heterozygous deletion requiring clinical reporting or a technical artifact attributable to sequence composition bias. Section 1: Installing the Integrative Genomics Viewer Overview of IGV The Integrative Genomics Viewer (IGV) is a genome browser application developed at the Broad Institute for visualization and interactive exploration of genomic data. IGV enables integration of multiple data types, including aligned sequencing reads, variant calls, and genomic annotations. For this analysis, you will use IGV to: Visualize read alignments in BAM format across the STK11 locus Quantify sequencing coverage depth at single-exon resolution Compare patient and control samples from the same sequencing run Identify supporting evidence such as heterozygous single nucleotide variants (or their absence in deleted regions) Step 1: Download IGV Windows Systems Access the download repository: Navigate to https://igv.org/doc/desktop/ using your web browser Locate the Downloads section Download the Windows installer: Select “IGV for Windows (installer)” File name format: IGV_Win_[version]_WithJava.exe Save to your Downloads directory macOS Systems Access the download repository: Navigate to https://igv.org/doc/desktop/ Download the macOS application: Select “IGV for macOS (app)” File name format: IGV_MacApp_[version]_[WithJava.zip](http://WithJava.zip) Save to Downloads directory Step 2: Installation Procedures Windows Installation Execute the installer: Navigate to Downloads directory Double-click IGV_Win_[version]_WithJava.exe Complete installation wizard: Advance through the welcome screen Accept the license agreement Default installation path: C:\\Program Files\\IGV_[version] Initiate installation (1-2 minutes) Launch IGV: Use the desktop shortcut or locate IGV via the Start menu Initial launch requires 15-30 seconds for Java initialization macOS Installation Locate the application file: Open Finder and navigate to Downloads Identify IGV_[version].app Transfer to Applications: Move IGV_[version].app to your Applications directory Initial launch procedure (critical for macOS security model): Attempt to launch IGV via double-click Verify successful installation: IGV should display its main interface Initial launch requires 15-30 seconds Step 3: Installation Verification Upon successful launch, the IGV interface displays: Menu bar (File, Genomes, View, Tracks, etc.) Toolbar containing genome version selector (typically defaulting to “Human hg19”) Primary visualization panel (initially empty) Reference gene track along the top margin Checkpoint: If you observe this interface configuration, installation is complete and functional. Troubleshooting Common Issues Java Runtime Not Found The recommended installers bundle the Java runtime. If you encounter a Java-related error: Verify you downloaded the “WithJava” installer variant Alternatively, install Java 11 or later macOS Gatekeeper Restrictions Follow the right-click procedure described above. Alternatively: Open System Preferences &gt; Security &amp; Privacy &gt; General Locate the IGV security notice and select “Open Anyway” Application Launch Failure First, verify that the available system memory meets the 4 GB minimum requirement. If memory is adequate but crashes persist: Locate the IGV installation directory Edit the launch script (igv.sh for macOS/Linux, igv.bat for Windows) Modify the heap memory parameter from -Xmx2g to -Xmx4g Section 2: Acquiring Case Study Data Files Binary Alignment Map (BAM) Files and Indices File Name Contents hereditary_case1.region.bam Patient aligned sequencing reads spanning the STK11 genomic region hereditary_case1.region.bam.bai Binary index for sample1 (required for random access by IGV) sample1.region.bam Control sample 1 aligned reads sample1.region.bam.bai Binary index for sample1 (samples 1-5 follow identical pattern) Additional control samples with corresponding indices Each BAM file requires its corresponding BAI index file in the same directory. IGV cannot efficiently access BAM files without valid indices. These files represent region-specific extracts rather than complete exome data. Full whole exome BAM files typically range from 5-50 GB per sample, which would be impractical for workshop distribution. The region extracts contain all reads mapping to the STK11 locus plus flanking sequence, preserving complete analytical capability while reducing file size. Browser Extensible Data (BED) Annotation Tracks File Name Annotation Content IWK_Caveats_11242022.bed Laboratory-specific regions with documented technical artefacts MANE_Select_hg19.bed MANE Select transcripts (clinically relevant canonical isoforms) median_coverage_1bp_exome.bed Expected coverage distribution across the exome capture at single-base resolution NCBI_GIAB_BP_problematic_hg19.bed Genome in a Bottle consortium problematic regions (difficult to sequence or align) Region_under_100X_median_average.bed Genomic intervals where median coverage typically falls below 100X Regions_median_coverage_under_20X_20MQ.bed Low coverage regions (below 20X depth with mapping quality threshold of 20) These annotation tracks will appear as colored intervals in IGV. Step 1: Accessing Data Files via Jupyter Notebook All case study data files are accessible through the Jupyter Notebook environment you are currently using. Locating the Data Directory: In your Jupyter Notebook interface, navigate to the file browser (typically the left sidebar) Locate the directory path: Case1_Hereditary/IGV/ You should see all BAM files, BAI indices, and BED annotation files listed Step 2: Local Directory Organization Before downloading files to your local machine, create your local workspace. Windows Systems: Open File Explorer Navigate to a location with adequate storage capacity (e.g., C:\\Users\\[YourUsername]\\Documents\\) Create the following directory hierarchy (suggested): BioinformaticsWorkshop\\ └── Module4_StructuralVariants\\ └── Case1_STK11\\ └── IGV_data\\ macOS/Linux Systems: Open Terminal and execute: cd ~/Documents mkdir -p BioinformaticsWorkshop/Module4_StructuralVariants/Case1_STK11/IGV_data cd BioinformaticsWorkshop/Module4_StructuralVariants/Case1_STK11/IGV_data Alternatively, use Finder (macOS) to create the directory structure manually. Step 3: Downloading Files from Jupyter to Your Local Machine Batch File Download via Jupyter Interface In the Jupyter file browser, navigate to Case1_Hereditary/IGV/ Select multiple files by holding Ctrl (Windows) or Command (macOS) while clicking on each file Select all required files: All BAM files (hereditary_case1.region.bam and sample1.region.bam through sample5.region.bam) All BAI index files (hereditary_case1.region.bam and sample1.region.bam.bai through sample5.region.bam.bai) All BED annotation files Right-click on any of the selected files and select “Download” from the contextual menu Your browser will download all selected files to your default Downloads directory Move the downloaded files to your organized workspace: BioinformaticsWorkshop/Module4_StructuralVariants/Case1_STK11/IGV_data/ Important Considerations: Ensure each BAM file has its corresponding BAI index file Keep BAM and BAI file pairs in the same directory Section 3: Loading Data into IGV and Navigating to STK11 We recommend to load data files into IGV in a specific order: Patient BAM file (hereditary_case1) Control BAM files (samples 1-5) BED annotation files When BAM files are loaded first, followed by BED files, IGV automatically positions the annotation tracks at the top of the visualization panel, with sequencing data tracks below. This organization facilitates visual comparison of patient and control samples. Note: Loading order is a matter of personal preference. If you prefer to load BED files first, they will remain in the order in which they were loaded, and you can manually rearrange the tracks later if needed. Step 1: Verifying Genome Build Selection Before loading any data files, confirm that IGV is configured to use the correct reference genome build. All analyses in this case study use the GRCh37/hg19 reference genome build. Using an incorrect genome build will result in misaligned data and invalid interpretations. To verify the genome build: Locate the genome selector dropdown in the IGV toolbar (upper left) The currently selected genome is displayed (typically defaults to “Human hg19”) If the selector does not display “Human hg19” or “Human (GRCh37/hg19)”: Click the dropdown menu Select “Human (GRCh37/hg19)” from the available options IGV will reload the reference genome (requires a few seconds) Step 2: Loading BAM Alignment Files BAM files contain the actual sequencing read alignments that form the basis of your coverage analysis. You will load all five samples (patient and controls) together. Loading all BAM files: Access the file loading interface: Select File &gt; Load from File Navigate to your data directory: Go to BioinformaticsWorkshop/Module4_StructuralVariants/Case1_STK11/IGV_data/ Select all BAM files: Hold Ctrl (Windows) or Command (macOS) while clicking Click on each BAM file: hereditary_case1.region.bam (patient sample) sample1.region.bam (control) sample2.region.bam (control) sample3.region.bam (control) sample4.region.bam (control) sample5.region.bam (control) All five files should be highlighted Important: Do not manually select the corresponding .bai index files. IGV will automatically detect and load the index files if they are present in the same directory as the BAM files. Load the files: Click “Open” IGV will load all five BAM files sequentially (~ 15-40 seconds for all files) Each BAM track consists of two components: Coverage track: A histogram displaying read depth across genomic positions Alignment track: Individual sequencing reads (visible only at high magnification) Step 3: Loading BED Annotation Files BED files provide genomic interval annotations that establish interpretive context for sequencing data. Loading these files after BAM files positions them at the top of the visualization panel for easy reference. Loading procedure: Access the file loading interface: Navigate to the menu bar Select File &gt; Load from File A file browser dialogue will open Navigate to your data directory: BioinformaticsWorkshop/Module4_StructuralVariants/Case1_STK11/IGV_data/ Select BED annotation files: Hold Ctrl (Windows) or Command (macOS) while clicking to select multiple files Select all six BED files: IWK_Caveats_11242022.bed MANE_Select_hg19.bed median_coverage_1bp_exome.bed NCBI_GIAB_BP_problematic_hg19.bed Region_under_100X_median_average.bed Regions_median_coverage_under_20X_20MQ.bed Complete the loading process: Click “Open” or “Load” IGV will process the files (typically 1-5 seconds per file) BED annotation tracks will appear at the top of the visualization panel Step 4: Navigating to the STK11 Gene Region After loading all data files, you will not immediately see coverage or alignment data. The region-specific BAM files contain sequencing data only for the STK11 locus, which represents a small fraction of the genome currently displayed. To visualize the data, you must navigate to the STK11 gene region. Locating the search interface: The search box is positioned in the IGV toolbar, immediately to the right of the chromosome selector dropdown (third box from the left in the toolbar): Enter the gene symbol: Click in the search box Type: STK11 Execute the search: Press Enter IGV will navigate to the STK11 gene locus on chromosome 19 The view will display the entire gene region, including all exons and introns Observe the initial visualization: Coverage histograms will become visible for all five samples BED annotation tracks will display intervals that overlap this region The reference gene track will show the STK11 gene structure Viewing the complete STK11 gene provides context before focusing on the suspected deletion. At this magnification level, you can: Assess overall coverage patterns across the entire gene Identify which exons show coverage reduction in the patient sample Observe the relationship between coverage patterns and gene structure Familiarize yourself with the data quality and characteristics for this case Step 5: Normalizing Coverage Scale Across Samples By default, IGV displays coverage histograms using an independent scale for each sample. Each coverage track is automatically scaled from 0 to its own maximum value. This setup creates issues challenges for comparative analysis. For example, if the patient sample has maximum coverage of 180X and a control sample has maximum coverage of 300X, both histograms will fill the available vertical space. Visual comparison becomes unreliable because the same histogram height represents different absolute coverage depths. To enable accurate visual comparison, you must normalize all coverage tracks to a common scale. Scale normalization procedure: Select all coverage tracks simultaneously: Hold Ctrl (Windows) or Command (macOS) Click on each coverage histogram track (the colored bar graph for each sample) Click on all five coverage tracks: the case1 sample, and sample1 through sample5 All selected tracks will be highlighted Access the scale configuration interface: Right-click on any of the selected coverage tracks A contextual menu will appear Select “Set Data Range” from the menu options Configure the uniform scale: A dialog box will appear with fields for minimum and maximum values In the “Minimum” field: Leave at 0 (default) In the “Maximum” field: Enter 327 This value represents the maximum coverage observed across all samples in this dataset Click “OK” to apply the scale Verify scale normalization: All coverage histograms should now use the same vertical scale The Y-axis labels should display identical ranges (0 to 327) for all samples Coverage differences between patient and control samples should now be visually apparent With normalized scales, histogram heights directly correspond to absolute coverage depths. A histogram at 50% height in the patient sample and 100% height in control samples immediately indicates approximately 50% coverage reduction, consistent with a heterozygous deletion. Without scale normalization, this visual interpretation would be impossible. Step 6: Focused Navigation to the Deletion Region The suspected deletion affects STK11 exons 6 and 7, spanning genomic coordinates chr19:1,221,191 to 1,222,025 (GRCh37/hg19 reference). For detailed analysis of coverage patterns and read alignments, navigate to this specific region: Enter deletion coordinates with flanking sequence: Click in the search box (replacing the current “STK11” text) Type: chr19:1,220,000-1,223,000 This range includes the deletion region plus approximately 1 kilobase of flanking sequence on each side Execute the navigation: Press Enter IGV will zoom to the specified region Individual exons will become clearly distinguishable Coverage patterns within exons 6 and 7 will be visible at high resolution Step 7: Verification After navigating to the STK11 region, verify that all data tracks are displaying correctly. Expected visualization elements: Reference genome track (top): Gene annotations for STK11 Exon and intron structures Transcript orientation (5’ to 3’ direction) BED annotation tracks: Colored intervals indicating various genomic features Some tracks may show intervals in this region, others may be empty Coverage histograms for all five samples: Vertical bars representing read depth at each genomic position Patient sample (case1) should display visibly reduced coverage in the deletion region Control samples (samples 1-5) should display consistent high coverage Alignment tracks (if zoomed sufficiently): Individual sequencing reads displayed as horizontal bars Become visible when viewing regions smaller than approximately 30 kilobases Common Issues and Troubleshooting Issue: BAM tracks show “Index not found” error Cause: The .bai index file is missing or not located in the same directory as the .bam file. Resolution: Verify that each BAM file has a corresponding BAI file with identical naming (e.g., hereditary_case1.region.bam and `hereditary_case1.region.bam.bai) Ensure both files are in the same directory Reload the BAM file Issue: Coverage appears as a flat line at zero Cause: You are viewing a genomic region that is not present in the region-specific BAM extract. Resolution: Verify you have navigated to chr19:1,220,000-1,223,000 Confirm the genome build is set to GRCh37/hg19 Check that BAM files loaded without error messages Issue: Genome coordinates do not match expected values Cause: IGV is using a different genome build (possibly GRCh38/hg38). Resolution: Change the genome selector to “Human (GRCh37/hg19)” Reload all data files Navigate again to the STK11 coordinates Issue: IGV performance is extremely slow Cause: Insufficient memory allocation or attempting to view too large a genomic region with all alignment details. Resolution: Close other memory-intensive applications Zoom to a smaller genomic region (100 kb or less) Increase IGV memory allocation as described in Section 1 troubleshooting Hide alignment tracks and view only coverage histograms Section 4: Quality Assessment of the Suspected Deletion Copy number variant detection from sequencing data usually requires quality assessment before clinical interpretation. Automated variant callers flag potential deletions based on statistical models, but these algorithms cannot fully account for technical artifacts arising from sequence complexity, capture efficiency, or sample quality. Your objective in this section is to systematically evaluate whether the suspected STK11 deletion represents: A true heterozygous deletion requiring clinical reporting and validation, or A technical artefact caused by sequencing bias, requiring variant call rejection This assessment follows the laboratory standard operating procedures used in clinical genomics laboratories. The evaluation encompasses four components: Coverage depth quantification and comparison Read mapping quality assessment Alignment pattern inspection Integration of supporting and contradictory evidence Step 1: Configuring IGV Display Settings for Coverage Analysis Collapsing alignment tracks to maximize coverage visibility: Individual alignment reads provide valuable information but consume substantial vertical space. During initial coverage assessment, collapsed alignment tracks improve efficiency. Select all alignment tracks: These appear below each coverage histogram Hold Ctrl/Command and click each alignment track Collapse the tracks: Right-click on any selected alignment track Select “Collapsed” from the visualization mode options Alignment tracks will compress to minimal height You can expand alignment tracks later when examining read-level evidence. Step 2: Quantitative Coverage Analysis Accurate coverage quantification requires measurement at specific genomic positions within the suspected deletion boundaries. You will record coverage values for both exons 6 and 7 across all five samples. Navigation to exon 6: Enter the following coordinates in the search box: chr19:1,221,191-1,221,500 Press Enter This region encompasses exon 6 of STK11 Measuring coverage depth: IGV displays coverage values dynamically as you move your cursor over the coverage histogram. Position your cursor over the coverage histogram for hereditary_case1 (patient): Move the cursor to the approximate center of the exon 6 region Observe the information box that appears near the cursor The box displays: genomic position, coverage depth at that position, and mapping quality statistics Record the coverage value: Note the coverage depth (displayed as an integer, e.g., “Coverage: 45”) Sample multiple positions across the exon (left, center, right) Calculate the approximate average coverage for exon 6 Repeat for all control samples (samples 1-5): For each control sample, measure coverage at the center of exon 6 Record all values Expected coverage pattern for a true heterozygous deletion: Patient sample: approximately 50% of control sample coverage Control samples: relatively consistent coverage depths (within 20-30% of each other) Example data recording table: Sample Exon 6 Coverage Exon 7 Coverage hereditary_case1 (patient) sample1 (control) sample2 (control) sample3 (control) sample4 (control) sample5 (control) Navigation to exon 7: Enter coordinates: chr19:1,221,750-1,222,025 Press Enter Repeat the coverage measurement process for exon 7 Record values in the table above Calculating the coverage ratio: For each exon, calculate the patient-to-control coverage ratio: Coverage ratio = (Patient coverage) / (Mean control coverage) For a heterozygous deletion, the expected ratio is approximately 0.5 (50% reduction). Example calculation: Patient exon 6 coverage: 45X Control mean exon 6 coverage: (107 + 134 + 150 + 164) / 4 = 139X Coverage ratio: 45 / 139 = 0.32 Interpretation of coverage ratios: Ratio 0.4-0.6: Consistent with heterozygous deletion Ratio 0.6-0.8: Intermediate, suggests possible technical bias Ratio 0.8-1.2: Coverage difference not consistent with deletion Ratio &lt;0.1 or &gt;1.8: Homozygous deletion or duplication Step 3: Mapping Quality Assessment Read mapping quality (MAPQ) quantifies the confidence that each sequencing read is aligned to the correct genomic location. Reads with low mapping quality may be mismapped, which can artificially reduce apparent coverage and create false positive deletion calls. MAPQ is expressed on a Phred-scaled logarithmic scale: MAPQ 60: 1 in 1,000,000 probability of incorrect mapping (very high confidence) MAPQ 40: 1 in 10,000 probability of incorrect mapping MAPQ 30: 1 in 1,000 probability of incorrect mapping MAPQ 20: 1 in 100 probability of incorrect mapping MAPQ 0-10: Ambiguous mapping, read may align equally well to multiple locations Laboratory quality thresholds: Minimum acceptable MAPQ: 20 (1% error probability) Recommended MAPQ: 30-60 for clinical variant calling Assessing mapping quality in IGV: Enable color coding by mapping quality: Navigate to View &gt; Preferences Select the “Alignments” tab Under “Color alignments by,” select “mapping quality” Click OK Interpret the color scheme: IGV colours reads on a gradient: Dark/bright colors: High MAPQ (good quality) Pale/gray colors: Low MAPQ (poor quality) Specific colours vary by IGV version, but usually intensity correlates with quality Visual inspection of the deletion region: Navigate to chr19:1,221,191-1,222,025 Expand alignment tracks by right-clicking and selecting “Expanded” Examine the reads present in the patient sample within the deletion region Assess whether the remaining reads (approximately 50% of normal if deletion is true) show high or low mapping quality Question: Are the reads that remain in the patient sample (within the deletion region) high quality or low quality? Interpretation: If remaining reads show high MAPQ (dark/bright colors): This supports a true deletion. The reads originate from the non-deleted allele and map with high confidence. If remaining reads show low MAPQ (pale/gray colors): This suggests technical artefact. Low quality reads may be mismapped or derived from repetitive sequence. Additional mapping quality check using the ruler: Hover your cursor over individual reads in the alignment track A tooltip displays read-specific information including MAPQ Sample 5-10 reads in the deletion region Verify that most reads exceed MAPQ 30 Step 4: Deletion Breakpoint Definition For a true structural deletion, the coverage reduction should have clearly defined boundaries corresponding to the deletion breakpoints. Gradual coverage transitions or irregular boundaries suggest technical artefacts. Visualizing coverage transitions: Navigate to the left deletion boundary: Enter coordinates: chr19:1,221,000-1,221,400 This spans the region from normal coverage into the deletion Assess the left breakpoint: Observe the transition from normal coverage (upstream) to reduced coverage (deletion region) A true deletion shows an abrupt transition in coverage (within 50-100 base pairs) GC bias or capture efficiency artifacts show gradual transitions (300-500+ base pairs) Navigate to the right deletion boundary: Enter coordinates: chr19:1,221,900-1,222,200 Assess the right breakpoint: Evaluate the transition from reduced coverage back to normal coverage Apply the same criteria as for the left breakpoint Characteristics of true deletion breakpoints: Sharp transitions in coverage depth (sudden drops and recoveries) Consistent breakpoint positions across multiple reads Presence of split reads or discordant read pairs at breakpoints (visible if zoom level is high enough) Characteristics of technical artifact boundaries: Gradual coverage transitions Irregular or poorly defined boundaries Absence of supporting structural variant evidence at transitions Step 5: Assessment of Problematic Genomic Regions The BED annotation tracks loaded earlier identify regions with known technical challenges. Coverage reductions that overlap with these problematic regions have higher probability of representing technical artefacts. Reviewing annotation tracks: Ensure you are viewing the region chr19:1,221,191-1,222,025 (the deletion coordinates). Examine each annotation track: IWK_Caveats_11242022.bed: Does this track show any colored intervals overlapping the deletion region? This track contains laboratory-specific regions with documented false-positive CNV calls If overlap exists: This region has produced false positives previously. If no overlap: No prior laboratory-specific issues documented. NCBI_GIAB_BP_problematic_hg19.bed: Does the Genome in a Bottle consortium identify this region as problematic? These regions have structural complexity, high homology to other genomic locations, or sequencing challenges If overlap exists: Independent evidence of technical difficulty. Support for artefact hypothesis. Region_under_100X_median_average.bed and Regions_median_coverage_under_20X_20MQ.bed: Do these tracks show intervals in the deletion region? These identify regions where exome capture efficiency is typically reduced If overlap exists: Coverage reduction may be expected technical variation rather than deletion. median_coverage_1bp_exome.bed: This track displays expected coverage distribution Compare the expected coverage to your observed control sample coverage If expected coverage is low: The region may be difficult to sequence regardless of deletion status. Interpretation framework: Annotation Pattern Interpretation No overlap with any problematic region tracks Coverage reduction is less likely to be technical artefact Overlap with one problematic region track Interpret cautiously; consider additional validation Overlap with multiple problematic region tracks High probability of technical artifact; variant call may be false positive Section 6: Evaluation of Supporting Genetic Evidence Beyond coverage patterns, additional genetic evidence can support or contradict a deletion hypothesis. Heterozygous SNVs in the deletion region: If the patient is heterozygous for single nucleotide variants (SNVs) within the deletion region, this contradicts the deletion hypothesis. A true deletion removes one allele, so SNVs should appear homozygous (or hemizygous, with only one allele visible). Visual inspection for SNVs: Navigate to chr19:1,221,191-1,222,025 Expand the alignment track for hereditary_case1 (patient) Zoom to high magnification if needed (right-click and zoom, or use the zoom slider) Scan the reads for positions showing color variation IGV colors nucleotides: A (green), C (blue), G (brown/orange), T (red) Heterozygous SNVs appear as mixed colors at a single genomic position Approximately 50% of reads show one color, 50% show another color Interpretation: If heterozygous SNVs are present in the deletion region: The second allele is intact. It contradicts the deletion hypothesis and suggests a technical artifact. If no heterozygous SNVs are present: This does not confirm deletion (absence of evidence is not evidence of absence), but it does not contradict the deletion hypothesis. If homozygous SNVs are present: This is consistent with a deletion (only one allele remains visible). Section 7: Integrating Evidence and Reaching a Quality Assessment Conclusion After completing the evaluation, integrate all evidence to reach a conclusion about deletion authenticity. Create a summary of your findings: Quality Assessment Criterion Observation Interpretation Coverage ratio (patient/control) [Your calculated ratio] [Consistent with deletion? Yes/No/Intermediate] Mapping quality of reads in deletion region [High/Low] [Supports true deletion / Suggests artefact] Deletion breakpoint sharpness [Abrupt/Gradual] [Supports deletion / Suggests artefact] Overlap with problematic region annotations [None / One track / Multiple tracks] [Low artefact risk / Moderate / High] Heterozygous SNVs in deletion region [Present/Absent] [Contradicts deletion / Consistent with deletion] GC content of deletion region [Percentage] [GC bias likely? Yes/No] Decision framework: Based on your evidence summary, select the most appropriate conclusion: High confidence true deletion: Coverage ratio 0.4-0.6 High mapping quality Sharp breakpoints No overlap with problematic regions No contradicting SNV evidence Decision: Proceed to clinical interpretation and validation planning Probable deletion, validation recommended: Coverage ratio 0.4-0.6 Some technical concerns present (GC bias, single problematic region overlap) Decision: Proceed to clinical interpretation, but emphasize validation requirement Uncertain, validation essential: Coverage ratio 0.5-0.8 Multiple technical concerns Decision: Defer clinical interpretation until orthogonal validation (e.g., MLPA) is completed Probable technical artefact: Coverage ratio &gt;0.7 Low mapping quality, gradual breakpoints, or contradicting SNV evidence Decision: Reject variant call, do not proceed to clinical interpretation Document your quality assessment conclusion with specific supporting evidence. In clinical practice, this documentation is included in the variant interpretation report. Proceeding to Clinical Variant Interpretation If your quality assessment supports a true deletion (categories 1 or 2 above), proceed to Section 5 for clinical interpretation using the ClinGen CNV Loss framework. If your assessment suggests a technical artifact (category 4), skip to the final discussion section. Section 5: Clinical Interpretation of Copy Number Loss Using ClinGen Guidelines Overview of ClinGen CNV Interpretation Framework After confirming that a copy number variant is likely genuine through quality assessment, the next critical step is determining its clinical significance. This requires systematic evaluation of: The genomic content of the deletion (which genes and regulatory elements are affected) The biological mechanism (haploinsufficiency, triplosensitivity, disruption of regulatory elements) Evidence from published literature and curated databases The patient’s clinical phenotype The Clinical Genome Resource (ClinGen) consortium has developed a standardized scoring framework for copy number variant interpretation. This rubric provides evidence-based criteria for classifying CNVs into five categories: Pathogenic Likely pathogenic Uncertain significance (VUS) Likely benign Benign The ClinGen framework is implemented as a technical standard by clinical laboratories worldwide and is the basis for CNV interpretation guidelines from the American College of Medical Genetics (ACMG). Accessing the ClinGen CNV Loss Calculator The ClinGen CNV interpretation process is facilitated by an interactive web-based tool that guides you through systematic evaluation of each evidence criterion. Access procedure: Open a web browser Navigate to: https://cnvcalc.clinicalgenome.org/cnvcalc/cnv-loss The ClinGen Dosage Sensitivity Curation page will load Select the “CNV Loss” calculator option Overview of the calculator interface: The calculator is organized into sections corresponding to ACMG/ClinGen scoring criteria: Section 1: Initial Assessment of Genomic Content (1A-1B) Section 2: Overlap with Established/Predicted HI or Established Benign Genes/Genomic Regions (2A-2H) Section 3: Evaluation of Gene Number Section 4: Detailed Evaluation of Genomic Content Using Published Literature, Public Databases, and/or Internal Lab Data (4A-4O) Section 5: Evaluation of Inheritance Pattern/Family History for Patient Being Studied (5A-5H) Each section contains specific evidence criteria with associated point values. The calculator automatically tallies points and suggests a classification based on accumulated evidence. Section 1 - Initial Assessment of Genomic Content Section 1 assesses what genes and genomic elements are affected by the deletion. The clinical significance of a CNV depends critically on the functional importance of the deleted genomic content. Criterion 1A: Does the CNV include any protein-coding or other critical genomic elements? Objective: Determine whether the deletion affects functionally important genomic sequences. Evaluation procedure: Identify genes within the deletion: The deletion at chr19:1,221,191-1,222,025 affects the STK11 gene Specifically, exons 6 and 7 are included in the deletion. Both exons are protein-coding sequence Assess functional importance: STK11 encodes serine/threonine kinase 11, a critical tumour suppressor The protein regulates cell polarity, metabolism, and growth control STK11 is definitively associated with Peutz-Jeghers syndrome (established gene-disease relationship) For this case: Continue evaluation Section 2 - Overlap with Established/Predicted HI or Established Benign Genes/Genomic Regions Section 2 evaluates whether the CNV overlaps with genes or genomic regions that have established or predicted haploinsufficiency (HI) or established benign status. This section is the most extensive portion of the ClinGen framework and evaluates whether the gene(s) affected by the CNV are sensitive to dosage imbalance. Criterion 2A: Does the CNV completely overlap an established HI gene or genomic region? Objective: Determine whether the deletion completely overlaps an established haploinsufficient gene or genomic region. Understanding complete overlap: Complete overlap means the deletion encompasses the entire established HI gene or critical region. This criterion applies when: The deletion includes all exons of a known HI gene from start to finish The deletion encompasses a complete established genomic region (e.g., 22q11.2 deletion syndrome region) For this case: There is no complete overlap Criterion 2B-2E: Overlap with Established/Predicted HI or Established Benign Genes/Genomic Regions These criteria evaluate partial gene overlaps: 2B: Partial overlap of an established HI genomic region 2C: Partial overlap with the 5’ end of an established HI gene 2D: Partial overlap with the 3’ end of an established HI gene 2E: Both breakpoints are within the same gene (intragenic deletion) Understanding haploinsufficiency (HI): Many genes tolerate loss of one allele with no phenotypic consequence because one functional copy produces adequate protein. However, haploinsufficient genes require both alleles for normal function. Loss of one allele results in disease through mechanisms including: Insufficient protein quantity (threshold effect) Disrupted stoichiometry in protein complexes Reduced compensatory capacity under cellular stress Evaluation of STK 11 HI Status: Review ClinGen haploinsufficiency curation: Navigate to: https://search.clinicalgenome.org/kb/gene-dosage?page=1&amp;size=25&amp;search Search for: STK11 Select the STK11 gene page Locate the “Dosage Sensitivity” section Interpret the haploinsufficiency (HI) score: 3: Sufficient evidence for dosage pathogenicity 2: Some evidence for dosage pathogenicity 1: Little evidence for dosage pathogenicity 0: No evidence for dosage pathogenicity 40: Dosage sensitivity unlikely For STK11: Haploinsufficiency score = 3 (sufficient evidence) For this case: The deletion affects exons 6-7, which are internal exons of STK11 (not at either terminal end). This constitutes an intragenic deletion falling under Criterion 2E. Assign 0.45-0.90 points Criterion 2F-G: Overlap with Established/Predicted HI or Established Benign Genes/Genomic Regions The STK11 deletion does not overlap any established benign CNV regions documented in population databases. Since STK11 is a well-established haploinsufficient gene with definitive clinical evidence, computational HI predictors (Criterion 2H) do not contribute additional scoring. Score: 0 points (continue evaluation). Section 3 - Evaluation of Gene Number Section 3 accounts for the number of genes affected by the deletion. Deletions affecting multiple genes may have additive effects on phenotype. For this case, the deletion affects one gene (STK11) Scoring for Section 3: The calculator provides a dropdown menu to select gene count. Point values increase with gene number: 0-24 genes: Variable points based on specific count 25-34 genes: Higher point values 35+ genes: Maximum point contribution For this case: Select: 3A 0-24 genes from dropdown menu. No points assigned Section 4 - Detailed Evaluation of Genomic Content Using Published Literature, Public Databases, and/or Internal Lab Data This section is used to evaluate literature and database evidence for genes or regions where haploinsufficiency has been reported but not yet formally established. For this case: The STK11 deletion overlaps an established haploinsufficient gene (ClinGen HI score = 3, scored in Section 2A). The scientific literature documenting pathogenic STK11 loss-of-function variants and deletions has already contributed to establishing STK11’s dosage sensitivity status and is reflected in the points assigned in Section 2A. To avoid double-counting evidence, we proceed directly to Section 5 As an excersice, here is how you can determine the presence of other sinilar clinically reported variants Navigate to the UCSC Genome Browser: Go to the UCSC Genome Browser on Human (GRCh37/hg19): https://genome.ucsc.edu/cgi-bin/hgTracks?db=hg19&amp;lastVirtModeType=default&amp;lastVirtModeExtraState=&amp;virtModeType=default&amp;virtMode=0&amp;nonVirtPosition=&amp;position=chr19%3A1221191-1222025&amp;hgsid=3296387915_3wxp93QQggeMSDa41nPThCuN6Dtf Verify you are viewing the correct genome build (GRCh37/hg19) and region chr19:1,221,191-1,222,025 Enable the ClinVar and Clingen tracks: Scroll down below the genome browser visualization to the track controls section Locate the section titled “Phenotype, Variants and Literature” Find the tracks labeled “ClinVar Variants” and “Clingen CNVs” Change the dropdown menu from “hide” to “pack” or “full” to make the track visible Click “refresh” to reload the browser with the track enabled Review ClinVar deletion entries: Once the track is visible, you should see colored annotations representing known ClinVar variants in the STK11 region Click on individual variant entries to view details Relevant pathogenic (red) deletions affecting exons 6-7 Section 5 - Evaluation of Inheritance Pattern/Family History for Patient Being Studied This section evaluates whether the CNV is de novo, inherited, or shows segregation/non-segregation patterns in the patient’s family. Points are assigned based on inheritance data and how well the patient’s phenotype matches established disease presentations. Understanding Section 5 criteria: Section 5 is most informative when detailed family information is available: De novo status (5A): Provides strong evidence for pathogenicity when the phenotype is specific and well-defined Inherited from unaffected parent (5B-5C): May suggest reduced penetrance, variable expressivity, or benign variation Segregation with disease (5D): Multiple affected family members with consistent phenotypes strengthen pathogenicity Non-segregation (5E): Finding the variant in unaffected family members provides evidence against pathogenicity Uninformative inheritance with specific phenotype (5G-5H): Can still contribute points when phenotype strongly matches the gene’s known disease For this case: Limited inheritance and family history information is available for this patient. This is a common situation in clinical genomics, where: Family members may not be available for testing Parental samples were not collected Pedigree information is incomplete Testing was performed as a singleton case However, we can still apply criterion 5H. Criterion 5H: Inheritance information unavailable or uninformative, with highly specific consistent phenotype Despite lacking detailed family data, the patient presents with clinical features highly specific for Peutz-Jeghers syndrome: Characteristic mucocutaneous pigmentation (perioral, buccal, acral) Gastrointestinal hamartomatous polyps Family history consistent with autosomal dominant inheritance pattern These features match the well-defined Peutz-Jeghers syndrome phenotype associated with STK11 haploinsufficiency. The phenotype is highly specific (The combination of pigmentation and GI polyps is pathognomonic), well-documented (extensive literature describes this presentation), and consistent (patient’s features align with published case descriptions) For this case: Assign: 0.30 points. Inheritance uninformative, but patient has highly specific phenotype consistent with similar cases Step 6: Calculating the Final Classification The ClinGen calculator automatically tallies point values from all scored criteria and suggests a classification. Point summary for this case: Section Points Section 1: Initial Assessment of Genomic Content Section 2: Overlap with Established HI Genes/Regions Section 3: Evaluation of Gene Number Section 4: Detailed Evaluation of Genomic Content Section 5: Evaluation of Inheritance Pattern/Family History Total ClinGen classification thresholds: The scoring framework uses the following point thresholds: ≥0.99 points: Pathogenic 0.90-0.98 points: Likely pathogenic 0.00-0.89 points: Uncertain significance (VUS) −0.90 to −0.01 points: Likely benign ≤−0.99 points: Benign Step 7: Clinical Recommendations and Validation You have now completed the interpretation of a copy number loss variant using the ClinGen framework. The process comprised: Quality assessment in IGV: Verification that the deletion call represents a true structural variant rather than technical artefact Systematic evidence evaluation: Scoring multiple evidence criteria addressing genomic content, dosage sensitivity, patient phenotype, and population data Classification: Integration of evidence to reach a pathogenic classification Clinical recommendations: Validation testing and clinical management planning This workflow represents the standard approach used in clinical genomics laboratories for copy number variant interpretation. The methodology ensures consistent, reproducible classifications and supports defensible clinical decision-making. Final activity: Synthesize your findings into a concise clinical summary that integrates the technical evidence with clinical context. Instructions: Write a 200-300 word summary that addresses the following: Variant description: Describe the STK11 deletion (genomic coordinates, size, affected exons) Classification and scoring: State your final classification (Pathogenic/Likely Pathogenic/VUS/Likely Benign/Benign) based on the ACMG/ClinGen CNV scoring framework. Report your total score and which sections contributed points. Clinical significance: Explain what this variant means for the patient’s diagnosis of Peutz-Jeghers syndrome Supporting evidence: Briefly summarize the key evidence types that supported your classification: Established haploinsufficiency of STK11 Patient phenotype consistency Any other relevant factors Format your summary as if it were part of a clinical laboratory report that would be interpreted by the ordering physician. References Primary Literature van Lier MG, Wagner A, Mathus-Vliegen EM, Kuipers EJ, Steyerberg EW, van Leerdam ME. High cancer risk in Peutz-Jeghers syndrome: a systematic review and surveillance recommendations. Am J Gastroenterol. 2010;105(6):1258-1265. doi:10.1038/ajg.2009.725. https://pubmed.ncbi.nlm.nih.gov/20051941/ Jelsig AM, Karstensen JG, Overeem Hansen TV. Progress report: Peutz–Jeghers syndrome. Familial Cancer. 2024;23:409-417. https://doi.org/10.1007/s10689-024-00362-7 Wang Z, Churchman M, Avizienyte E, et al. Germline mutations of the LKB1 (STK11) gene in Peutz-Jeghers patients. J Med Genet. 1999;36(5):365-368. https://pmc.ncbi.nlm.nih.gov/articles/PMC1734361/ Volikos E, Robinson J, Aittomäki K, et al. LKB1 exonic and whole gene deletions are a common cause of Peutz-Jeghers syndrome. J Med Genet. 2006;43(5):e18. https://ncbi.nlm.nih.gov/pmc/articles/PMC2564523/ Software and Databases Robinson JT, Thorvaldsdóttir H, Winckler W, et al. Integrative Genomics Viewer. Nat Biotechnol. 2011;29(1):24-26. IGV software available at: https://igv.org/doc/desktop/ ClinGen Dosage Sensitivity Map. Clinical Genome Resource. Available at: https://search.clinicalgenome.org/kb/gene-dosage?page=1&amp;size=25&amp;search ClinGen CNV Interpretation Calculator. Clinical Genome Resource. Available at: https://cnvcalc.clinicalgenome.org/cnvcalc/cnv-loss UCSC Genome Browser (GRCh37/hg19). University of California Santa Cruz Genomics Institute. Available at: https://genome.ucsc.edu/cgi-bin/hgTracks?db=hg19&amp;lastVirtModeType=default&amp;lastVirtModeExtraState=&amp;virtModeType=default&amp;virtMode=0&amp;nonVirtPosition=&amp;position=chr19%3A1221191-1222025&amp;hgsid=3296387915_3wxp93QQggeMSDa41nPThCuN6Dtf Guidelines and Standards Riggs ER, Andersen EF, Cherry AM, et al. Technical standards for the interpretation and reporting of constitutional copy-number variants: a joint consensus recommendation of the American College of Medical Genetics and Genomics (ACMG) and the Clinical Genome Resource (ClinGen). Genet Med. 2020;22(2):245-257. "],["module-4-lab-microsatellite-instability-visualization.html", "Module 4 Lab: Microsatellite Instability Visualization Learning Objectives Lab Overview Background: Understanding Microsatellite Instability Part 1: Data Download from Jupyter Notebook Part 2: Loading Data into IGV Part 3: Visualizing Microsatellite Instability Summary and Clinical Significance", " Module 4 Lab: Microsatellite Instability Visualization Date: November 5, 2025 Summary: The lab focuses on visualizing microsatellite instability (MSI) patterns in tumor versus normal tissue using IGV, emphasizing the significance of allelic heterogeneity and its clinical applications, including Lynch syndrome screening and immunotherapy selection. Participants will download and analyze BAM files, observe instability at specific microsatellite loci, and understand the implications of MSI in cancer diagnostics and treatment. Text: Microsatellite Instability Visualization Lab Overview Learning Objectives By the end of this lab, you will: Visualize and interpret microsatellite instability patterns in tumor versus normal tissue using IGV Recognize allelic heterogeneity as the molecular signature of mismatch repair deficiency Connect MSI testing to clinical applications including Lynch syndrome screening and immunotherapy selection Lab Overview Duration: 20 minutes Microsatellite instability (MSI) is a hypermutator phenotype caused by defective DNA mismatch repair. MSI-high tumors accumulate insertion and deletion mutations at repetitive DNA sequences (microsatellites), creating a distinctive molecular signature visible in sequencing data. This lab uses data from a colorectal cancer case with confirmed MSI-high status (See reference at the end). You’ll visualize specific microsatellite loci where the tumor shows allelic heterogeneity compared to the patient’s normal tissue. Background: Understanding Microsatellite Instability Microsatellites are repetitive DNA sequences (1-6 base pair motifs) that constitute approximately 3% of the human genome. Common types include mononucleotide repeats like (A)n, dinucleotide repeats like (CA)n, and higher-order repeats. During DNA replication, polymerase can slip when copying repetitive sequences, creating insertion or deletion errors. In normal cells, the mismatch repair (MMR) system (comprising MLH1, MSH2, MSH6, and PMS2 proteins) recognizes and corrects these errors, reducing microsatellite mutation rates by 100-1000 fold. When MMR is deficient (via germline pathogenic variants in Lynch syndrome or somatic MLH1 hypermethylation in sporadic tumors), polymerase slippage errors accumulate uncorrected across thousands of microsatellites genome-wide. This creates a hypermutator phenotype with mutation rates 100-1000 times higher than microsatellite-stable tumors. Unlike typical clonal driver mutations, MSI manifests as multiple different indel alleles at each microsatellite locus. Each tumor subclone independently acquires random polymerase slippage errors, creating a characteristic “pile-up” appearance in sequencing data with insertions, deletions, and varying allele fractions. Normal tissue shows uniform read alignment because MMR efficiently corrects rare errors. Part 1: Data Download from Jupyter Notebook Accessing the Data Directory Open your JupyterHub session in a web browser Navigate to the course data directory: Module4/Microsatellite/ Verify you can see the following files: test.normal.bam test.normal.bam.bai test.tumor.bam test.tumor.bam.bai Downloading the BAM Files Download the tumor sample: Right-click on test.tumor.bam Select “Download” Save to a location you can easily access (e.g., Desktop or Downloads folder) Download the tumor index: Right-click on test.tumor.bam.bai Select “Download” Save to the same folder as the tumor BAM file Download the normal sample: Right-click on test.normal.bam Select “Download” Save to the same folder as the tumor files Download the normal index: Right-click on test.normal.bam.bai Select “Download” Save to the same folder as the other files ⚠️Requirement: The BAI index files must be in the same directory as their corresponding BAM files. IGV will not load alignments without the index files present. File organization check: Your download folder should now contain exactly four files: test.tumor.bam test.tumor.bam.bai test.normal.bam test.normal.bam.bai Part 2: Loading Data into IGV Installing and Launching IGV If you haven’t already installed IGV from a previous lab session, refer to the previous case for installation instructions. Quick launch: Open IGV on your computer Wait for the application to fully load (you should see the reference genome selector in the top left) Ensure you’re using the hg19 reference genome to match the alignment. In the top left corner, locate the genome dropdown menu If it doesn’t already show “Human hg19”, click the dropdown Select “Human (hg19)” from the list Wait for IGV to load the reference genome (this may take 10-15 seconds) Loading the BAM Files Load the normal sample: Click “File” → “Load from File…” Navigate to your download folder Select test.normal.bam Click “Open” IGV will automatically detect and use the corresponding .bai index file Load the tumor sample: Click “File” → “Load from File…” again Select test.tumor.bam Click “Open” You should now see two tracks in the IGV viewer: test.normal.bam (top track) test.tumor.bam (bottom track)  Tip: You can reorder tracks by clicking and dragging the track name labels on the left side. Keep normal on top and tumor on bottom for easier comparison. Part 3: Visualizing Microsatellite Instability You’ll now navigate to a highly MSI-sensitive locus on chromosome 1 where the tumor shows instability compared to normal tissue. Step 1: Navigate to the Microsatellite Locus In IGV’s search box at the top of the screen, paste the coordinates below: chr1:16265055-16265085 Press Enter to jump to the locus. You should see the reference sequence displaying: Left flank: AAAGC Microsatellite: T repeated 19 times (T×19) Right flank: CATTC  Confirming the flanking sequences (AAAGC on the left, CATTC on the right) ensures you’re viewing the correct microsatellite locus. This T×19 mononucleotide tract is one of 19 consecutive thymine bases in the reference genome. Step 2: Configure IGV Track Display Settings To optimize visualization of microsatellite instability patterns, adjust the alignment track settings: For both the tumour and normal BAM tracks: Right-click on the alignment track name (either test.tumor.bam or test.normal.bam) and select “Expanded” view mode (if not already selected) Turn on “Show center line” This helps visualize read continuity and gaps ⚠️IGV performance note: If IGV becomes slow with downsampling disabled, you can re-enable it at a higher threshold (e.g., 1000 reads instead of the default 50). The key is to see enough reads to observe the heterogeneous indel pattern in the tumor. Step 3: Observe and Compare MSI Patterns Now examine the alignment patterns in the T×19 microsatellite region. Focus on the differences between tumor and normal tissue. Normal tissue track (test.normal.bam): Reads align smoothly through the poly-T tract Little to no indel pile-up: You may see 1-2 isolated indel marks (sequencing errors), but no clustering All reads show the same T×19 structure Grey bars indicate reads matching the reference genome Tumor tissue track (test.tumor.bam): Pile-up of deletions: Look for clustered colored tick marks (typically black or dark) within the T-run These indicate multiple reads with deletions relative to the reference The ticks appear as short vertical lines interrupting the grey read bars Variable indel sizes: Different reads show different deletion lengths Some reads may have -1 T deletion (18 Ts instead of 19) Others may have -2, -3, or larger deletions You may also see occasional insertions (less common than deletions) Heterogeneous appearance: The “noisy” pattern reflects multiple tumor subclones with independent slippage errors White gaps in reads: Deletions appear as empty spaces where bases are missing (connected by a black line, and the number of nucleotides involved. Step 4: Understanding What You’re Seeing Why mononucleotide tracts are MSI-sensitive: Poly-T tracts like this T×19 microsatellite are the most unstable repeat type in MMR-deficient tumors: DNA polymerase frequently slips during replication of long homopolymer runs Each slippage event creates a small insertion or deletion loop Without functional MMR, these errors accumulate uncorrected Each tumor cell lineage acquires different random slippage errors → allelic heterogeneity Why you see deletion bias: At most mononucleotide repeats, deletions outnumber insertions due to the mechanics of polymerase slippage: Template strand looping (forward slippage) creates deletions more frequently Nascent strand looping (backward slippage) creates insertions less frequently Clinical significance of this single locus: Even observing instability at this one microsatellite strongly suggests genome-wide MMR deficiency. In MSI-high tumors, thousands of similar loci show comparable patterns across the genome. Step 5: Lab Deliverable Take a screenshot of your IGV view showing both tracks (tumor and normal) with the microsatellite region visible. Your screenshot should clearly show: The chromosome 1 coordinates in the search box Both test.normal.bam and test.tumor.bam tracks The contrasting patterns: uniform alignment in normal vs. indel pile-up in tumor The reference sequence track (if visible) showing the T×19 repeat To capture the screenshot: Windows: Use the Snipping Tool or press Windows+Shift+S Mac: Press Command+Shift+4 and drag to select the IGV window Linux: Use Screenshot utility or press PrtScn Save the screenshot with a descriptive filename (e.g., MSI_chr1_T19_comparison.png) Summary and Clinical Significance By comparing tumor and normal alignments at microsatellite loci, you’ve visualized: Allelic heterogeneity: Multiple different indel patterns at the same genomic position Tumor-specific instability: Normal tissue maintains stable repeat lengths, while tumor tissue shows widespread variation Frameshift accumulation: Insertions and deletions create reading frame disruptions in coding sequences Clinical Applications of MSI Testing 1. Diagnostic classification: MSI-high (MSI-H): Instability at ≥30% of tested loci MSI-low (MSI-L): Instability at &lt;30% of loci Microsatellite stable (MSS): No instability detected 2. Lynch syndrome screening: MSI-high tumors in young patients suggest germline mismatch repair defects Triggers reflex testing for MLH1, MSH2, MSH6, PMS2 mutations 3. Immunotherapy selection: MSI-high tumors are highly responsive to immune checkpoint inhibitors (anti-PD-1/PD-L1) FDA-approved indication: pembrolizumab for MSI-H/dMMR solid tumors Response rates: 40-60% in MSI-H colorectal cancer vs. &lt;5% in MSS tumors 4. Prognostic stratification: MSI-high colorectal cancers have better stage-adjusted prognosis May not benefit from 5-fluorouracil chemotherapy (standard for MSS tumors) References Ziegler, J., Hechtman, J.F., Rana, S. et al. A deep multiple instance learning framework improves microsatellite instability detection from tumor next generation sequencing. Nat Commun 16, 136 (2025). "]]
